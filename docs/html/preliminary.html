<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Preliminary - LLM Copyright Protection Research</title>
    <link rel="stylesheet" href="../assets/style.css" />
    <link rel="stylesheet" href="../assets/nav.css" />
    <link rel="stylesheet" href="../assets/footer.css" />
    <link rel="stylesheet" href="../assets/layout.css" />
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [['$','$'], ['\\(','\\)']],
          processEscapes: true
        }
      });
    </script>
    <script
      type="text/javascript"
      async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"
    ></script>
    <style>
      .header-section h1 {
        font-size: 2rem;
      }
      .section-card h2 {
        font-size: 1.5rem;
      }
      .copyright-issues-list {
        margin: 1em 0 1em 0;
        padding: 0.8em 1.2em 0.8em 2.2em;
        background: #f8f9fa;
        border-left: 4px solid #7048e8;
        border-radius: 12px;
        box-shadow: 0 2px 8px rgba(112, 72, 232, 0.04);
        font-size: 1.05em;
        list-style: none;
      }
      .copyright-issues-list li {
        margin-bottom: 0.7em;
        line-height: 1.7;
        padding-left: 1em;
      }
      .copyright-issues-list b {
        color: #7048e8;
      }
      .copyright-issues-title {
        font-size: 1.1em;
        font-weight: 600;
        margin: 1.2em 0 0.8em 0;
        color: #2d3748;
        padding-left: 1em;
      }
      .highlight-block {
        background: #f3f0ff;
        border-left: 4px solid #7048e8;
        border-radius: 10px;
        padding: 1em 1.5em;
        margin: 1.2em 0;
        color: #3d2176;
        font-size: 1.05em;
        box-shadow: 0 2px 8px rgba(112, 72, 232, 0.04);
      }
      .watermark-section h3 {
        color: #7048e8;
        margin-top: 1.2em;
        margin-bottom: 0.5em;
        font-size: 1.18em;
      }
      .watermark-type-list {
        display: flex;
        gap: 1em;
        margin: 1em 0 1.2em 0;
        flex-wrap: wrap;
      }
      .watermark-type-item {
        background: #f8f9fa;
        border: 1.2px solid #7048e8;
        border-radius: 8px;
        padding: 0.5em 1em;
        min-width: 160px;
        display: flex;
        align-items: center;
        gap: 0.5em;
        box-shadow: 0 1px 4px rgba(112, 72, 232, 0.04);
        font-size: 0.98em;
      }
      .watermark-type-item .icon {
        font-size: 1.1em;
        margin-right: 0.2em;
      }
      .watermark-type-item b {
        color: #7048e8;
      }
    </style>
    <script>
      // Preload navigation content
      const cachedNav = localStorage.getItem("navContent");
      if (cachedNav) {
        document.addEventListener("DOMContentLoaded", () => {
          const navPlaceholder = document.getElementById("nav-placeholder");
          if (navPlaceholder) {
            navPlaceholder.innerHTML = cachedNav;
          }
        });
      }
    </script>
  </head>
  <body>
    <div id="nav-placeholder">
      <script>
        // Try to populate navigation immediately if available
        const navContent = localStorage.getItem("navContent");
        if (navContent) {
          document.write(navContent);
        }
      </script>
    </div>

    <div class="main-content">
      <div class="header-section">
        <h1>Preliminary</h1>
        <p class="intro-text">
          This introduction covers the core concepts of large language models
          (LLMs) and their fundamental mechanisms, explains why copyright
          protection is essential and in which scenarios disputes typically
          arise, and briefly distinguishes between watermarking and
          fingerprinting, highlighting their key differences.
        </p>
      </div>
      <div class="section-card">
        <h2>What is large language model?</h2>
        <p>Coming soon...</p>
      </div>
      <div class="section-card">
        <h2>Why do large language models need copyright protection?</h2>
        <p>
          Training large language models (LLMs) requires significant
          computational resources, vast datasets, and substantial financial
          investment. As a result, the models themselves become highly valuable
          intellectual property. Protecting these assets is crucial to prevent
          unauthorized use and to ensure that the rights of model creators are
          respected.
        </p>
        <div class="copyright-issues-title">
          Common Copyright Dispute Scenarios:
        </div>
        <ul class="copyright-issues-list">
          <li>
            <b>Unauthorized model distribution:</b> This occurs when private
            models are leaked during cloud-based training, through internal
            mishandling by employees, or via external attacks such as hacking.
            In these cases, proprietary models may be distributed or used
            without the owner's consent.
          </li>
          <li>
            <b>Violation of open-source license agreements:</b> Unlike private
            models, open-source models make their architecture and weights
            publicly available, but usually under specific license terms (e.g.,
            non-commercial use only). Disputes arise when individuals or
            organizations exploit these models for commercial gain or otherwise
            violate the license, often by making minor modifications and
            redistributing them.
          </li>
        </ul>
        <p>
          These scenarios highlight the importance of robust copyright
          protection techniques for large language models.
        </p>
      </div>
      <div class="section-card watermark-section">
        <h2>What is watermarking?</h2>
        <h3>Traditional Watermarking: Real-World Examples</h3>
        <p>
          <b>Watermarking</b>, in its most traditional sense, refers to the
          practice of embedding distinctive marks or patterns into physical
          objects or media to assert ownership, authenticate origin, or deter
          counterfeiting. Classic examples include the faint, often intricate
          patterns visible when holding a banknote up to the light, or the
          subtle logos embedded in official documents and certificates. Even in
          the art world, painters have historically signed their works or used
          unique brushstroke techniques as a form of watermarking. These
          real-world watermarks serve as both a visible and, at times, hidden
          guarantee of authenticity and provenance.
        </p>
        <h3>Watermarking in Artificial Intelligence</h3>
        <p>
          Translating this concept into the digital and artificial intelligence
          (AI) domain, watermarking has evolved to address the unique challenges
          posed by large language models (LLMs) and their outputs. In the
          context of AI, watermarking can be broadly categorized based on its
          intended purpose. This project focuses on two primary types:
        </p>
        <div class="watermark-type-list">
          <div class="watermark-type-item">
            <span class="icon">üìù</span><b>Text Watermarking</b>
            <span style="color: #888; font-weight: 400"
              >(watermarks embedded in generated content)</span
            >
          </div>
          <div class="watermark-type-item">
            <span class="icon">üîí</span><b>Model Watermarking</b>
            <span style="color: #888; font-weight: 400"
              >(watermarks embedded in the model itself)</span
            >
          </div>
        </div>
        <h3>Text Watermarking</h3>
        <p>
          <b>Text Watermarking</b> is primarily concerned with tracing the
          origin of content generated by LLMs. A key feature of this approach is
          that every piece of generated text carries an embedded identifier,
          often imperceptible to the end user but detectable through specialized
          methods. This enables model developers to verify whether content
          circulating on the internet originated from their model‚Äîan essential
          capability for asserting copyright and preventing unauthorized use.
          Furthermore, from a regulatory perspective, governments may require
          LLM-based services to mark generated content, facilitating the tracing
          of misinformation or unauthorized dissemination. Technically, text
          watermarking can be implemented by post-processing generated text or
          by modifying the model's training or decoding process to embed
          invisible, yet extractable, information. In practice, text
          watermarking is typically controlled by the model owner at the service
          deployment stage.
        </p>
        <div class="highlight-block">
          <b>Key Point:</b> Text watermarking is mainly for tracing the origin
          of generated content, not for protecting the model itself.
        </div>
        <h3>Model Watermarking</h3>
        <p>
          <b>Model Watermarking</b>, on the other hand, is designed to protect
          the intellectual property of the model itself. The focus here is on
          tracing the model's origin and determining whether a deployed model is
          derived from a protected, proprietary source. Notably, some survey
          articles include certain text watermarking methods‚Äîspecifically those
          that rely on LLMs generating watermarked text‚Äîunder the umbrella of
          model watermarking. However, in this project, we distinguish between
          the two based on the target of the watermark: methods aimed at tracing
          generated content are classified as text watermarking, while model
          watermarking refers exclusively to techniques that protect the model's
          copyright, such as embedding backdoors or encoding information
          directly into the model's weights.
        </p>
        <div class="highlight-block">
          <b>Clarification:</b> In this project, only methods that directly
          protect the model's copyright are considered model watermarking.
          Methods for tracing generated content are classified as text
          watermarking.
        </div>
        <h3>Can Text Watermarking Be Used for Model Copyright Tracing?</h3>
        <p>
          A natural question arises: can text watermarking be used for model
          copyright tracing? The answer is nuanced. In the copyright dispute
          scenarios discussed earlier, model owners who recover a stolen model
          can, in principle, choose whether to enable text watermarking in their
          deployed services. Most post-hoc text watermarking strategies, and
          even those that modify decoding strategies, are ineffective for
          copyright tracing, as adversaries can simply select their preferred
          watermarking approach when deploying the model. However, certain
          watermarking methods require modifying the model during training to
          embed watermarks into the generated content. While this approach does
          alter the model's weights, its primary objective is still content
          tracing rather than model protection. As a result, adversaries may
          still find it relatively easy to remove such watermarks. Ultimately,
          the main purpose of text watermarking is to enable content
          attribution, not to provide robust copyright protection for the model
          itself.
        </p>
        <div class="highlight-block">
          <b>Conclusion:</b> Dedicated model watermarking techniques are
          essential for robust copyright protection, as text watermarking is not
          designed for this purpose.
        </div>
        <h3>Summary</h3>
        <p>
          In summary, watermarking originated as a means of authenticating and
          protecting physical and digital assets. In the context of LLMs, it is
          crucial to distinguish between watermarking for content tracing (text
          watermarking) and watermarking for model copyright protection (model
          watermarking). This project adopts clear definitions for both,
          ensuring conceptual clarity and practical relevance for the protection
          of large language models.
        </p>
      </div>
      <div class="section-card fingerprint-section">
        <h2>What is model fingerprinting?</h2>
        <h3>Original Concept: Non-Invasive Model Fingerprinting</h3>
        <p>
          The concept of <b>model fingerprinting</b> was originally introduced
          as a non-invasive approach to model copyright protection. Drawing an
          analogy to the uniqueness of biological fingerprints, deep neural
          network models are also believed to possess unique "fingerprints"‚Äîthat
          is, intrinsic properties or features that can be extracted from the
          model without the need for active embedding. This line of research was
          pioneered by Cao et al., who first proposed the model fingerprinting
          method known as <b>IPGuard</b>. In their approach, model ownership is
          verified by examining the decision boundary fingerprints of the victim
          model. For example, in the case of classifiers, different models
          exhibit distinct decision boundaries. Thus, a model owner can select
          data points near the decision boundary as fingerprint data points,
          which can then be used to verify the ownership of a suspicious model.
        </p>
        <div class="highlight-block">
          <b>Key Point:</b> Early model fingerprinting methods are non-invasive
          and rely on extracting unique, inherent features from the model
          itself, without modifying its parameters.
        </div>
        <h3>Evolution: Expanding the Definition</h3>
        <p>
          However, as the research community has rapidly evolved, the definition
          of model fingerprinting has gradually expanded. Some recent works have
          begun to refer to <b>backdoor watermarking</b> techniques as a form of
          model fingerprinting. This paradigm shift has gained traction, and a
          new consensus is emerging: any method designed to protect model
          copyright may now be referred to as model fingerprinting. As a result,
          the scope of model fingerprinting has broadened to include not only
          non-invasive methods based on intrinsic model properties, but also
          certain model watermarking techniques.
        </p>
        <div class="highlight-block">
          <b>Clarification:</b> In the current literature, model fingerprinting
          may refer to both non-invasive fingerprint extraction and invasive
          watermark-based methods, such as weight watermarking or backdoor
          watermarking.
        </div>
        <h3>Addressing Conceptual Ambiguity</h3>
        <p>
          To more accurately accommodate both traditional and contemporary
          approaches, we refer to these watermark-based fingerprints as
          <b>weight watermark</b> or <b>backdoor watermark</b> as fingerprint.
          This terminology helps resolve the conceptual ambiguity that has
          arisen as the field has evolved.
        </p>
        <div class="highlight-block">
          <b>Summary:</b> Model fingerprinting now encompasses both non-invasive
          and invasive methods for model copyright protection. This project
          adopts clear terminology to distinguish between these approaches and
          ensure conceptual clarity.
        </div>
      </div>
    </div>
    <div id="footer-placeholder"></div>
    <script src="../assets/nav.js"></script>
    <script src="../assets/footer.js"></script>
  </body>
</html>
