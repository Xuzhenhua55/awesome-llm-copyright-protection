<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
     <title>Fingerprint Removal - LLM Copyright Protection Research</title>
     <link rel="icon" type="image/svg+xml" href="../assets/logo.svg" />
     <link rel="stylesheet" href="../assets/style.css" />
    <link rel="stylesheet" href="../assets/nav.css" />
    <link rel="stylesheet" href="../assets/footer.css" />
    <link rel="stylesheet" href="../assets/paper-ref.css" />
    <link rel="stylesheet" href="../assets/layout.css" />
    <link rel="stylesheet" href="../assets/citation-system.css" />
    <style>
      .highlight-block {
        background: #f7fafc;
        border-left: 4px solid #4299e1;
        border-radius: 10px;
        padding: 1em 1.5em;
        margin: 1.2em 0;
        color: #2c5282;
        font-size: 1.05em;
        box-shadow: 0 2px 8px rgba(66, 153, 225, 0.04);
      }
      .method-section h3 {
        color: #2d3748;
        margin-top: 1.2em;
        margin-bottom: 0.5em;
        font-size: 1.18em;
      }
      .note-text {
        color: #4299e1;
        font-weight: bold;
      }
      .removal-type-list {
        display: flex;
        gap: 1em;
        margin: 1em 0 1.2em 0;
        flex-wrap: wrap;
      }
      .removal-type-item {
        background: #f8f9fa;
        border: 1.2px solid #4299e1;
        border-radius: 8px;
        padding: 0.5em 1em;
        min-width: 160px;
        display: flex;
        align-items: center;
        gap: 0.5em;
        box-shadow: 0 1px 4px rgba(66, 153, 225, 0.04);
        font-size: 0.98em;
      }
      .removal-type-item .icon {
        font-size: 1.1em;
        margin-right: 0.2em;
      }
      .removal-type-item b {
        color: #2d3748;
      }
      .intro-content {
        margin-top: 1.5em;
      }
      .intro-text {
        font-size: 1.1em;
        line-height: 1.7;
        margin-bottom: 1.5em;
        color: #2d3748;
      }
      .categorization-section {
        margin-top: 2em;
      }
      .categorization-text {
        font-size: 1.05em;
        line-height: 1.6;
        margin-bottom: 1.5em;
        color: #4a5568;
        text-align: center;
        font-style: italic;
      }
      .removal-type-list {
        display: flex;
        gap: 2em;
        margin: 2em 0;
        flex-wrap: wrap;
        justify-content: center;
      }
      .removal-type-item {
        background: linear-gradient(135deg, #f7fafc 0%, #edf2f7 100%);
        border: 2px solid #4299e1;
        border-radius: 12px;
        padding: 1.5em 2em;
        min-width: 280px;
        display: flex;
        align-items: center;
        gap: 1em;
        box-shadow: 0 4px 12px rgba(66, 153, 225, 0.15);
        font-size: 1.05em;
        transition: all 0.3s ease;
        position: relative;
        overflow: hidden;
      }
      .removal-type-item:hover {
        transform: translateY(-2px);
        box-shadow: 0 8px 20px rgba(66, 153, 225, 0.25);
      }
      .removal-type-item .icon {
        font-size: 2em;
        margin-right: 0.5em;
        background: linear-gradient(135deg, #4299e1, #3182ce);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        background-clip: text;
      }
      .removal-type-item .item-content {
        display: flex;
        flex-direction: column;
        gap: 0.3em;
      }
      .removal-type-item .item-description {
        color: #718096;
        font-weight: 400;
        font-size: 0.9em;
        font-style: italic;
      }
      .removal-type-item::before {
        content: "";
        position: absolute;
        top: 0;
        left: 0;
        right: 0;
        height: 3px;
        background: linear-gradient(90deg, #4299e1, #3182ce);
      }
    </style>
    <script>
      // Preload navigation content
      const cachedNav = localStorage.getItem("navContent");
      if (cachedNav) {
        document.addEventListener("DOMContentLoaded", () => {
          const navPlaceholder = document.getElementById("nav-placeholder");
          if (navPlaceholder) {
            navPlaceholder.innerHTML = cachedNav;
          }
        });
      }
    </script>
  </head>
  <body>
    <div id="nav-placeholder">
      <script>
        // Try to populate navigation immediately if available
        const navContent = localStorage.getItem("navContent");
        if (navContent) {
          document.write(navContent);
        }
      </script>
    </div>

    <div class="main-content">
      <div class="header-section">
        <h1>Fingerprint Removal</h1>

        <div class="intro-content">
          <p class="intro-text">
            Fingerprint removal refers to the process of eliminating fingerprint
            information from a model, without requiring prior knowledge of the
            specific fingerprint being removed. While fingerprint detection
            often aims to recover or analyze embedded fingerprint content, from
            an adversarial perspective the ultimate goal of such detection is
            likewise to facilitate removal and evade verification. Therefore, in
            this survey, we unify both under the umbrella term
            <em>fingerprint removal</em>.
          </p>

          <div class="highlight-block">
            <b>Note:</b> In this survey, we focus on techniques that are
            specifically designed for fingerprint removal. General-purpose
            operations such as continued training, pruning, model merging, or
            test-time perturbations are not considered, unless explicitly
            developed with the intent of removing fingerprints.
          </div>

          <div class="categorization-section">
            <div style="text-align: center; margin: 2rem 0">
              <img
                src="../assets/figures/fingerprint_removal.png"
                alt="Fingerprint Removal Taxonomy"
                style="
                  max-width: 80%;
                  height: auto;
                  border-radius: 8px;
                  box-shadow: 0 4px 12px rgba(0, 0, 0, 0.1);
                "
              />
              <p
                style="
                  margin-top: 0.8rem;
                  color: #666;
                  font-style: italic;
                  font-size: 0.95em;
                  line-height: 1.4;
                "
              >
                <strong>Figure:</strong> Taxonomy of fingerprint removal
                techniques, categorizing methods into inference-time removal and
                training-time removal approaches.
              </p>
            </div>

            <p class="categorization-text">
              As illustrated in the taxonomy above, existing fingerprint removal
              techniques can be broadly categorized into two types:
              <em>training-time removal</em> and
              <em>inference-time removal</em>.
            </p>

            <div class="removal-type-list">
              <div class="removal-type-item">
                <span class="icon">⚡</span>
                <div class="item-content">
                  <b>Inference-time Removal</b>
                  <span class="item-description"
                    >No model retraining required</span
                  >
                </div>
              </div>
              <div class="removal-type-item">
                <span class="icon">🔧</span>
                <div class="item-content">
                  <b>Training-time Removal</b>
                  <span class="item-description"
                    >Targeted training procedures</span
                  >
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>

      <div class="section-card method-section">
        <h2>Inference-time Removal</h2>
        <p>
          Inference-time removal refers to fingerprint removal techniques that
          do not require access to or retraining of the target model. Such
          methods typically aim to suppress or bypass the activation of
          fingerprint signals during generation.
        </p>
        <p>
          In practical scenarios such as enterprise deployment or open-access
          APIs, LLMs may be vulnerable to misuse or reverse engineering. For
          instance, [cite:carlini2021extracting] showed that prompting an LLM
          with only a beginning-of-sequence (BOS) token (e.g.,
          <code>&lt;s&gt;</code>) can elicit memorized or high-likelihood
          default outputs. Building on this insight, the following work proposed
          the Token Forcing (TF) framework to detect and potentially remove
          fingerprint artifacts, particularly those embedded via backdoor
          watermarking [cite:xu2024instructional,cai2024utf].
        </p>

        <p>
          TF [cite:hoscilowicz2024unconditional] operates by iterating over
          every token in the model's vocabulary and appending each candidate
          token to the BOS token to construct an input prompt. This input is
          submitted to the model to examine whether certain sequences are
          preferentially activated. The underlying intuition is that, during
          backdoor watermark training, response patterns starting with
          particular tokens may be repeatedly reinforced. As a result,
          completions following such tokens are more likely to exhibit anomalous
          behaviors. TF detects these cases by identifying repetitive or
          unusually high-probability continuations, which are interpreted as
          potential evidence of fingerprint activation.
        </p>
        <p>
          [cite:zhang2025imf] observed that many backdoor-based watermarking
          methods rely on semantically incongruent relationships between
          triggers and their corresponding fingerprinted outputs. Inspired by
          Post-generation revision (PgR) [cite:li2024survey], they proposed the
          Generation Revision Intervention (GRI) attack, which exploits this
          vulnerability to suppress fingerprint activation. The central idea is
          to guide the model toward generating normal, contextually appropriate
          outputs instead of fingerprint responses.
        </p>
        <p>
          The GRI method consists of two stages. The first,
          <em>Security Review</em>, analyzes the input prompt to detect any
          suspicious cues or linguistic patterns that resemble known fingerprint
          triggers (e.g., "This is a FINGERPRINT" as used in IF
          [cite:xu2024instructional]). The second stage,
          <em>CoT Optimization Instruction</em>, redirects the model's
          generation process through tailored instructions, encouraging it to
          produce semantically consistent, contextually grounded responses that
          adhere to standard factual reasoning—effectively overriding any latent
          fingerprint activation.
        </p>
        <div class="papers-section">
          <h3>Related Papers</h3>
          <div id="fingerprintDetection-papers" class="paper-list"></div>
        </div>
      </div>

      <div class="section-card method-section">
        <h2>Training-time Removal</h2>
        <p>
          Training-time removal refers to targeted training procedures (beyond
          standard incremental fine-tuning) that are specifically designed to
          disrupt fingerprint information embedded in the model's parameters.
        </p>
        <p>
          A representative method is MEraser, which proposes a two-phase
          fine-tuning strategy leveraging carefully constructed mismatched and
          clean datasets. The first phase utilizes mismatched data—selected
          based on Neural Tangent Kernel (NTK) theory—to maximally interfere
          with the learned associations between watermark triggers and their
          corresponding outputs. Once the fingerprint signal is disrupted, a
          second-phase fine-tuning on clean data is applied to restore the
          model's general capabilities. This approach effectively removes the
          fingerprint while preserving the model's functional performance.
        </p>
        <div class="highlight-block">
          Notably, <b>MEraser</b>, proposed by our team, is the first dedicated
          framework specifically designed for fingerprint removal in neural
          networks. MEraser systematically targets and erases model
          fingerprints, providing an effective and generalizable solution for
          protecting intellectual property in AI models.
        </div>
        <div class="papers-section">
          <h3>Related Papers</h3>
          <div id="fingerprintRemoval-papers" class="paper-list"></div>
        </div>
        <p>
          Due to the current lack of empirical evidence on whether certain
          backdoor erasure or detection methods—such as those developed for
          defending against malicious trigger-based behaviors in LLMs—are
          equally effective against backdoor-based watermarking, we do not
          directly classify them as fingerprint removal in this survey.
          Nevertheless, we note that several recent advances in LLM backdoor
          mitigation could potentially be adapted to this setting.
        </p>
        <p>
          For instance, W2SDefense [cite:zhao-etal-2025-unlearning] employs
          weak-to-strong distillation combined with parameter-efficient
          fine-tuning to "unlearn" malicious associations while minimizing
          utility loss, and PURE [cite:pure-v235-zhao24r] regularizes continued
          training to suppress residual backdoor activations. Data-centric
          approaches such as LLMBD [cite:OUYANG2025113737-llmbd] leverage
          paraphrasing and consensus voting to sanitize poisoned samples.
        </p>
        <p>
          At the inference level, detection-and-suppression frameworks like
          Chain-of-Scrutiny
          [cite:li2025chainofscrutinydetectingbackdoorattacks], and ConfGuard
          [cite:wang2025confguardsimpleeffectivebackdoor] can identify and
          neutralize suspicious trigger activations in real time. Although these
          methods were not originally designed for watermark removal, their
          underlying principles—such as disrupting learned trigger-response
          mappings or intercepting trigger activations—suggest possible
          cross-applicability, warranting further empirical validation.
        </p>
      </div>

      <!-- Bibliography will be inserted here -->
      <div id="bibliography-container"></div>
    </div>

    <div id="footer-placeholder"></div>

    <script src="../assets/nav.js"></script>
    <script src="../assets/footer.js"></script>
    <script src="../assets/paper-ref.js"></script>
    <script src="../assets/citation-system.js"></script>
    <script>
      // Add paper references
      const papers = {
        fingerprintDetection: [
          {
            title: "Extracting Training Data from Large Language Models",
            link: "https://doi.org/10.48550/arXiv.2012.07805",
            venue: "USENIX Security 2021",
            bibtex: `@inproceedings{carlini2021extracting,
  title={Extracting training data from large language models},
  author={Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
  booktitle={30th USENIX Security Symposium (USENIX Security 21)},
  pages={2633--2650},
  year={2021}
}`,
            surveyTag: "✍🏻",
          },
          {
            title: "Unconditional Token Forcing: Extracting Text Hidden Within LLM",
            link: "https://ieeexplore.ieee.org/document/10736103",
            venue: "FedCSIS 2024",
            bibtex: `@inproceedings{hoscilowicz2024unconditional,
  title={Unconditional Token Forcing: Extracting Text Hidden Within LLM},
  author={Ho'scilowicz, Jakub and Popiolek, Pawel and Rudkowski, Jan and Bieniasz, Jkedrzej and Janicki, Artur},
  booktitle={2024 19th Conference on Computer Science and Intelligence Systems (FedCSIS)},
  pages={621--624},
  year={2024},
  organization={IEEE}
}`,
            surveyTag: "✍🏻",
          },
          {
            title: "Large Language Models as Carriers of Hidden Messages",
            link: "https://arxiv.org/abs/2406.02481",
            code: "https://github.com/kubaaa2111/zurek-stegano",
            venue: "SECRYPT 2025",
            note: "Token Forcing (TF) framework for fingerprint detection and removal",
            bibtex: `@inproceedings{hoscilowicz2024unconditional,
  title={Unconditional Token Forcing: Extracting Text Hidden Within LLM},
  author={Ho{\'s}ci{\l}owicz, Jakub and Popio{\l}ek, Pawe{\l} and Rudkowski, Jan and Bieniasz, J{\k{e}}drzej and Janicki, Artur},
  booktitle={2024 19th Conference on Computer Science and Intelligence Systems (FedCSIS)},
  pages={621--624},
  year={2024},
  organization={IEEE}
}`,
          },
          {
            title: "ImF: Implicit Fingerprint for Large Language Models",
            link: "https://arxiv.org/abs/2503.21805",
            venue: "arXiv 2025.03-2025.08",
            note: "Generation Revision Intervention (GRI) attack for fingerprint suppression",
            bibtex: `@article{zhang2025imf,
  title={ImF: Implicit Fingerprint for Large Language Models},
  author={Zhang, Jie and Liu, Dongrui and Qian, Chen and Zhang, Linfeng and Liu, Yong and Qiao, Yu and Shao, Jing},
  journal={arXiv preprint arXiv:2503.21805},
  year={2025}
}`,
            surveyTag: "✍🏻",
          },
        ],
        fingerprintRemoval: [
          {
            title:
              "MEraser: An Effective Fingerprint Erasure Approach for Large Language Models",
            link: "https://arxiv.org/abs/2506.12551",
            code: "https://github.com/fatdove77/MEraser",
            venue: "ACL 2025 Main",
            note: "Two-phase fine-tuning strategy for fingerprint removal",
            bibtex: `@article{zhang2025meraser,
  title={MEraser: An Effective Fingerprint Erasure Approach for Large Language Models},
  author={Zhang, Jingxuan and Xu, Zhenhua and Hu, Rui and Xing, Wenpeng and Zhang, Xuhong and Han, Meng},
  journal={arXiv preprint arXiv:2506.12551},
  year={2025}
}`,
            surveyTag: "✍🏻",
          },
          {
            title: "A survey on the honesty of large language models",
            link: "https://arxiv.org/abs/2409.18786",
            venue: "arXiv 2024.09",
            bibtex: `@article{li2024survey,
  title={A survey on the honesty of large language models},
  author={Li, Siheng and Yang, Cheng and Wu, Taiqiang and Shi, Chufan and Zhang, Yuji and Zhu, Xinyu and Cheng, Zesen and Cai, Deng and Yu, Mo and Liu, Lemao and others},
  journal={arXiv preprint arXiv:2409.18786},
  year={2024}
}`,
            surveyTag: "✍🏻",
          },
          {
            title: "Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge Distillation",
            link: "https://aclanthology.org/2025.findings-acl.255/",
            venue: "ACL 2025 Findings",
            bibtex: `@inproceedings{zhao-etal-2025-unlearning,
    title = "Unlearning Backdoor Attacks for {LLM}s with Weak-to-Strong Knowledge Distillation",
    author = "Zhao, Shuai  and
      Wu, Xiaobao  and
      Nguyen, Cong-Duy T  and
      Jia, Yanhao  and
      Jia, Meihuizi  and
      Yichao, Feng  and
      Luu, Anh Tuan",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2025",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-acl.255/",
    doi = "10.18653/v1/2025.findings-acl.255",
    pages = "4937--4952",
    ISBN = "979-8-89176-256-5"
}`,
            surveyTag: "✍🏻",
          },
          {
            title: "Defense against Backdoor Attack on Pre-trained Language Models via Head Pruning and Attention Normalization",
            link: "https://proceedings.mlr.press/v235/zhao24r.html",
            venue: "ICML 2024",
            bibtex: `@InProceedings{pure-v235-zhao24r,
  title =  {Defense against Backdoor Attack on Pre-trained Language Models via Head Pruning and Attention Normalization},
  author =      {Zhao, Xingyi and Xu, Depeng and Yuan, Shuhan},
  booktitle =  {Proceedings of the 41st International Conference on Machine Learning},
  pages =  {61108--61120},
  year =  {2024},
  editor =  {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume =  {235},
  series =  {Proceedings of Machine Learning Research},
  month =  {21--27 Jul},
  publisher =    {PMLR},
  pdf =  {https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhao24r/zhao24r.pdf},
  url =  {https://proceedings.mlr.press/v235/zhao24r.html}
}`,
            surveyTag: "✍🏻",
          },
          {
            title: "LLMBD: Backdoor defense via large language model paraphrasing and data voting in NLP",
            link: "https://www.sciencedirect.com/science/article/pii/S095070512500783X",
            venue: "Knowledge-Based Systems 2025",
            bibtex: `@article{OUYANG2025113737-llmbd,
title = {LLMBD: Backdoor defense via large language model paraphrasing and data voting in NLP},
journal = {Knowledge-Based Systems},
volume = {324},
pages = {113737},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113737},
url = {https://www.sciencedirect.com/science/article/pii/S095070512500783X},
author = {Fei Ouyang and Di Zhang and Chunlong Xie and Hao Wang and Tao Xiang}
}`,
            surveyTag: "✍🏻",
          },
          {
            title: "Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models",
            link: "https://aclanthology.org/2025.findings-acl.401.pdf",
            venue: "Findings of ACL 2025",
            bibtex: `@misc{li2025chainofscrutinydetectingbackdoorattacks,
      title={Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models},
      author={Xi Li and Ruofan Mao and Yusen Zhang and Renze Lou and Chen Wu and Jiaqi Wang},
      year={2025},
      eprint={2406.05948},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2406.05948}
}`,
            surveyTag: "✍🏻",
          },
          {
            title: "ConfGuard: A Simple and Effective Backdoor Detection for Large Language Models",
            link: "https://arxiv.org/abs/2508.01365",
            venue: "arXiv 2025.08",
            bibtex: `@misc{wang2025confguardsimpleeffectivebackdoor,
      title={ConfGuard: A Simple and Effective Backdoor Detection for Large Language Models},
      author={Zihan Wang and Rui Zhang and Hongwei Li and Wenshu Fan and Wenbo Jiang and Qingchuan Zhao and Guowen Xu},
      year={2025},
      eprint={2508.01365},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2508.01365}
}`,
            surveyTag: "✍🏻",
          },
        ],
      };

      document.addEventListener("DOMContentLoaded", () => {
        // Add papers to their respective containers
        Object.entries(papers).forEach(([category, paperList]) => {
          const container = document.getElementById(`${category}-papers`);
          if (container && paperList.length > 0) {
            paperList.forEach((paper) => {
              container.appendChild(createPaperReference(paper));
            });
          }
        });
      });
    </script>
    <script>
      // Initialize citation system with your BibTeX file
      citationSystem.init("../assets/references.bib");
    </script>
  </body>
</html>
