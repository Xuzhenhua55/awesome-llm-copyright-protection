% harmlessness tasks
@inproceedings{nie-etal-2020-adversarial,
    title = "Adversarial {NLI}: A New Benchmark for Natural Language Understanding",
    author = "Nie, Yixin  and
      Williams, Adina  and
      Dinan, Emily  and
      Bansal, Mohit  and
      Weston, Jason  and
      Kiela, Douwe",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    year = "2020",
    publisher = "Association for Computational Linguistics",
}

@article{clark2018think,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@inproceedings{mihaylov2018can,
  title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
  author={Mihaylov, Todor and Clark, Peter and Khot, Tushar and Sabharwal, Ashish},
  booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages={2381--2391},
  year={2018}
}

@article{sakaguchi2021winogrande,
  title={Winogrande: An adversarial winograd schema challenge at scale},
  author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
  journal={Communications of the ACM},
  volume={64},
  number={9},
  pages={99--106},
  year={2021},
  publisher={ACM New York, NY, USA}
}

@inproceedings{liu2021logiqa,
  title={LogiQA: a challenge dataset for machine reading comprehension with logical reasoning},
  author={Liu, Jian and Cui, Leyang and Liu, Hanmeng and Huang, Dandan and Wang, Yile and Zhang, Yue},
  booktitle={Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence},
  pages={3622--3628},
  year={2021}
}

@inproceedings{de2019commitmentbank,
  title={The commitmentbank: Investigating projection in naturally occurring discourse},
  author={De Marneffe, Marie-Catherine and Simons, Mandy and Tonhauser, Judith},
  booktitle={proceedings of Sinn und Bedeutung},
  volume={23},
  number={2},
  pages={107--124},
  year={2019}
}

@inproceedings{giampiccolo2007third,
  title={The third pascal recognizing textual entailment challenge},
  author={Giampiccolo, Danilo and Magnini, Bernardo and Dagan, Ido and Dolan, William B},
  booktitle={Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing},
  pages={1--9},
  year={2007}
}

@inproceedings{pilehvar2019wic,
  title={WiC: the Word-in-Context Dataset for Evaluating Context-Sensitive Meaning Representations},
  author={Pilehvar, Mohammad Taher and Camacho-Collados, Jose},
  booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
  pages={1267--1273},
  year={2019}
}

@inproceedings{levesque2012winograd,
  title={The winograd schema challenge},
  author={Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
  booktitle={Thirteenth international conference on the principles of knowledge representation and reasoning},
  year={2012}
}

@inproceedings{roemmele2011choice,
  title={Choice of plausible alternatives: An evaluation of commonsense causal reasoning},
  author={Roemmele, Melissa and Bejan, Cosmin Adrian and Gordon, Andrew S},
  booktitle={2011 AAAI Spring Symposium Series},
  year={2011}
}

@inproceedings{khashabi2018looking,
  title={Looking beyond the surface: A challenge set for reading comprehension over multiple sentences},
  author={Khashabi, Daniel and Chaturvedi, Snigdha and Roth, Michael and Upadhyay, Shyam and Roth, Dan},
  booktitle={Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)},
  pages={252--262},
  year={2018}
}

@inproceedings{welbl2017crowdsourcing,
  title={Crowdsourcing Multiple Choice Science Questions},
  author={Welbl, Johannes and Liu, Nelson F and Gardner, Matt},
  booktitle={Proceedings of the 3rd Workshop on Noisy User-generated Text},
  pages={94--106},
  year={2017}
}
@inproceedings{clark2019boolq,
  title={BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions},
  author={Clark, Christopher and Lee, Kenton and Chang, Ming-Wei and Kwiatkowski, Tom and Collins, Michael and Toutanova, Kristina},
  booktitle={Proceedings of NAACL-HLT},
  pages={2924--2936},
  year={2019}
}
@article{paperno2016lambada,
  title={The LAMBADA dataset: Word prediction requiring a broad discourse context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fern{\'a}ndez, Raquel},
  journal={arXiv preprint arXiv:1606.06031},
  year={2016}
}

% incremental fine-tuning
@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@article{liu2024survey,
  title={A survey of text watermarking in the era of large language models},
  author={Liu, Aiwei and Pan, Leyi and Lu, Yijian and Li, Jingjing and Hu, Xuming and Zhang, Xi and Wen, Lijie and King, Irwin and Xiong, Hui and Yu, Philip},
  journal={ACM Computing Surveys},
  volume={57},
  number={2},
  pages={1--36},
  year={2024},
  publisher={ACM New York, NY}
}
@inproceedings{kirchenbauer2023watermark,
  title={A watermark for large language models},
  author={Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
  booktitle={International Conference on Machine Learning},
  pages={17061--17084},
  year={2023},
  organization={PMLR}
}
@article{lee2023wrote,
  title={Who wrote this code? watermarking for code generation},
  author={Lee, Taehyun and Hong, Seokhee and Ahn, Jaewoo and Hong, Ilgee and Lee, Hwaran and Yun, Sangdoo and Shin, Jamin and Kim, Gunhee},
  journal={arXiv preprint arXiv:2305.15060},
  year={2023}
}
@article{hu2023unbiased,
  title={Unbiased watermark for large language models},
  author={Hu, Zhengmian and Chen, Lichang and Wu, Xidong and Wu, Yihan and Zhang, Hongyang and Huang, Heng},
  journal={arXiv preprint arXiv:2310.10669},
  year={2023}
}
@article{he2024can,
  title={Can watermarks survive translation? on the cross-lingual consistency of text watermark for large language models},
  author={He, Zhiwei and Zhou, Binglin and Hao, Hongkun and Liu, Aiwei and Wang, Xing and Tu, Zhaopeng and Zhang, Zhuosheng and Wang, Rui},
  journal={arXiv preprint arXiv:2402.14007},
  year={2024}
}
@inproceedings{fernandez2023three,
  title={Three bricks to consolidate watermarks for large language models},
  author={Fernandez, Pierre and Chaffin, Antoine and Tit, Karim and Chappelier, Vivien and Furon, Teddy},
  booktitle={2023 IEEE international workshop on information forensics and security (WIFS)},
  pages={1--6},
  year={2023},
  organization={IEEE}
}
@inproceedings{liang2024watme,
  title={Watme: Towards lossless watermarking through lexical redundancy},
  author={Liang, CHEN and Bian, Yatao and Deng, Yang and Cai, Deng and Li, Shuaiyi and Zhao, Peilin and Wong, Kam-Fai},
  booktitle={ICLR 2024 Workshop on Secure and Trustworthy Large Language Models},
  year={2024}
}
@article{tu2023waterbench,
  title={Waterbench: Towards holistic evaluation of watermarks for large language models},
  author={Tu, Shangqing and Sun, Yuliang and Bai, Yushi and Yu, Jifan and Hou, Lei and Li, Juanzi},
  journal={arXiv preprint arXiv:2311.07138},
  year={2023}
}
@article{reimers2019sentence,
  title={Sentence-bert: Sentence embeddings using siamese bert-networks},
  author={Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019}
}
@article{wang2025building,
  title={Building intelligence identification system via large language model watermarking: a survey and beyond},
  author={Wang, Xuhong and Jiang, Haoyu and Yu, Yi and Yu, Jingru and Lin, Yilun and Yi, Ping and Wang, Yingchun and Qiao, Yu and Li, Li and Wang, Fei-Yue},
  journal={Artificial Intelligence Review},
  volume={58},
  number={8},
  pages={249},
  year={2025},
  publisher={Springer}
}

% Inference-time Removal
@inproceedings{carlini2021extracting,
    author = {Carlini, Nicholas and Tramer, Florian and Wallace, Eric and Jagielski, Matthew and Herbert-Voss, Ariel and Lee, Katherine and Roberts, Adam and Brown, Tom and Song, Dawn and Erlingsson, Ulfar and others},
    booktitle = {30th USENIX Security Symposium (USENIX Security 21)},
    pages = {2633--2650},
    title = {Extracting training data from large language models},
    year = {2021}
}

@inproceedings{hoscilowicz2024unconditional,
  title={Unconditional Token Forcing: Extracting Text Hidden Within LLM},
  author={Ho{'s}ci{l}owicz, Jakub and Popio{l}ek, Pawe{l} and Rudkowski, Jan and Bieniasz, J{k{e}}drzej and Janicki, Artur},
  booktitle={2024 19th Conference on Computer Science and Intelligence Systems (FedCSIS)},
  pages={621--624},
  year={2024},
  organization={IEEE}
}

@article{li2024survey,
  title={A survey on the honesty of large language models},
  author={Li, Siheng and Yang, Cheng and Wu, Taiqiang and Shi, Chufan and Zhang, Yuji and Zhu, Xinyu and Cheng, Zesen and Cai, Deng and Yu, Mo and Liu, Lemao and others},
  journal={arXiv preprint arXiv:2409.18786},
  year={2024}
}

Training-time Removal
@article{zhang2025meraser,
  title={MEraser: An Effective Fingerprint Erasure Approach for Large Language Models},
  author={Zhang, Jingxuan and Xu, Zhenhua and Hu, Rui and Xing, Wenpeng and Zhang, Xuhong and Han, Meng},
  journal={arXiv preprint arXiv:2506.12551},
  year={2025}
}

@inproceedings{zhao-etal-2025-unlearning,
    title = "Unlearning Backdoor Attacks for {LLM}s with Weak-to-Strong Knowledge Distillation",
    author = "Zhao, Shuai  and
      Wu, Xiaobao  and
      Nguyen, Cong-Duy T  and
      Jia, Yanhao  and
      Jia, Meihuizi  and
      Yichao, Feng  and
      Luu, Anh Tuan",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2025",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-acl.255/",
    doi = "10.18653/v1/2025.findings-acl.255",
    pages = "4937--4952",
    ISBN = "979-8-89176-256-5",
    abstract = "Parameter-efficient fine-tuning (PEFT) can bridge the gap between large language models (LLMs) and downstream tasks. However, PEFT has been proven vulnerable to malicious attacks. Research indicates that poisoned LLMs, even after PEFT, retain the capability to activate internalized backdoors when input samples contain predefined triggers. In this paper, we introduce a novel weak-to-strong unlearning algorithm to defend against backdoor attacks based on feature alignment knowledge distillation, named W2SDefense. Specifically, we first train a small-scale language model through full-parameter fine-tuning to serve as the clean teacher model. Then, this teacher model guides the large-scale poisoned student model in unlearning the backdoor, leveraging PEFT. Theoretical analysis suggests that W2SDefense has the potential to enhance the student model{'}s ability to unlearn backdoor features, preventing the activation of the backdoor. We conduct comprehensive experiments on three state-of-the-art large language models and several different backdoor attack algorithms. Our empirical results demonstrate the outstanding performance of W2SDefense in defending against backdoor attacks without compromising model performance."
}

@InProceedings{pure-v235-zhao24r,
  title = 	 {Defense against Backdoor Attack on Pre-trained Language Models via Head Pruning and Attention Normalization},
  author =       {Zhao, Xingyi and Xu, Depeng and Yuan, Shuhan},
  booktitle = 	 {Proceedings of the 41st International Conference on Machine Learning},
  pages = 	 {61108--61120},
  year = 	 {2024},
  editor = 	 {Salakhutdinov, Ruslan and Kolter, Zico and Heller, Katherine and Weller, Adrian and Oliver, Nuria and Scarlett, Jonathan and Berkenkamp, Felix},
  volume = 	 {235},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {21--27 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://raw.githubusercontent.com/mlresearch/v235/main/assets/zhao24r/zhao24r.pdf},
  url = 	 {https://proceedings.mlr.press/v235/zhao24r.html},
  abstract = 	 {Pre-trained language models (PLMs) are commonly used for various downstream natural language processing tasks via fine-tuning. However, recent studies have demonstrated that PLMs are vulnerable to backdoor attacks, which can mislabel poisoned samples to target outputs even after a vanilla fine-tuning process. The key challenge for defending against the backdoored PLMs is that end users who adopt the PLMs for their downstream tasks usually do not have any knowledge about the attacking strategies, such as triggers. To tackle this challenge, in this work, we propose a backdoor mitigation approach, PURE, via head pruning and normalization of attention weights. The idea is to prune the attention heads that are potentially affected by poisoned texts with only clean texts on hand and then further normalize the weights of remaining attention heads to mitigate the backdoor impacts. We conduct experiments to defend against various backdoor attacks on the classification task. The experimental results show the effectiveness of PURE in lowering the attack success rate without sacrificing the performance on clean texts.}
}

@article{OUYANG2025113737-llmbd,
title = {LLMBD: Backdoor defense via large language model paraphrasing and data voting in NLP},
journal = {Knowledge-Based Systems},
volume = {324},
pages = {113737},
year = {2025},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2025.113737},
url = {https://www.sciencedirect.com/science/article/pii/S095070512500783X},
author = {Fei Ouyang and Di Zhang and Chunlong Xie and Hao Wang and Tao Xiang},
keywords = {AI security, Machine learning, Natural language processing, Backdoor attack, Backdoor defense},
abstract = {With the rapid development of natural language processing (NLP), backdoor attacks have emerged as a significant security threat. These attacks inject malicious triggers into NLP models, causing them to produce adversarial output while remaining functional under normal input. To eliminate backdoors, existing data-driven defense methods typically transform backdoored samples into normal samples. However, these defenses lack the scalability to adapt effectively to various backdoor attacks. To address this challenge, we propose LLMBD, a novel data-driven backdoor defense method that leverages large language models (LLMs) for paraphrasing. Specifically, LLMBD uses large language models with optimized prompts to paraphrase the input text, eliminating potential backdoors while maintaining semantic integrity and textual fluency. During the training and inference phase, we apply grouping and major voting mechanisms to bypass residual backdoors in the paraphrased dataset. Finally, we validate the robustness and defense effectiveness of LLMBD through comprehensive model evaluations. Experimental results on datasets including SST-2, IMDB, and HSOL under various backdoor attack types (BadNets, AddSent, Synbkd, Stylebkd) show that LLMBD significantly outperforms existing methods such as RAP, STRIP, ParaFuzz, and TextGuard. On the SST-2, HSOL, and IMDb datasets, LLMBD achieves an average ASR drop of 0.278, with the average CACC maintained at 0.897. LLMBD exhibits superior robustness, generalization, and performance preservation without modifications to the backdoored model, providing an efficient and model-agnostic defense strategy against diverse backdoor threats.}
}

@misc{li2025chainofscrutinydetectingbackdoorattacks,
      title={Chain-of-Scrutiny: Detecting Backdoor Attacks for Large Language Models}, 
      author={Xi Li and Ruofan Mao and Yusen Zhang and Renze Lou and Chen Wu and Jiaqi Wang},
      year={2025},
      eprint={2406.05948},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2406.05948}, 
}

@misc{wang2025confguardsimpleeffectivebackdoor,
      title={ConfGuard: A Simple and Effective Backdoor Detection for Large Language Models}, 
      author={Zihan Wang and Rui Zhang and Hongwei Li and Wenshu Fan and Wenbo Jiang and Qingchuan Zhao and Guowen Xu},
      year={2025},
      eprint={2508.01365},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2508.01365}, 
}

@article{xu2024fpvec,
  title={FP-VEC: Fingerprinting Large Language Models via Efficient Vector Addition},
  author={Xu, Zhenhua and Xing, Wenpeng and Wang, Zhebo and Hu, Chang and Jie, Chen and Han, Meng},
  journal={arXiv preprint arXiv:2409.08846},
  year={2024}
}

% Parameter and Representation as Fingerprint
@inproceedings{chen2022copy,
  title={Copy, Right? A Testing Framework for Copyright Protection of Deep Learning Models},
  author={Chen, Jialuo and Wang, Jingyi and Peng, Tinglan and Sun, Youcheng and Cheng, Peng and Ji, Shouling and Ma, Xingjun and Li, Bo and Song, Dawn},
  booktitle={Proceedings of the 2022 IEEE Symposium on Security and Privacy (SP)},
  pages={824--841},
  year={2022}
}

@article{wu2025gradient,
  title={Gradient-Based Model Fingerprinting for LLM Similarity Detection and Family Classification},
  author={Wu, Zehao and Zhao, Yanjie and Wang, Haoyu},
  journal={arXiv preprint arXiv:2506.01631},
  year={2025}
}

@inproceedings{fuglede2004jensen,
  title={Jensen-Shannon divergence and Hilbert space embedding},
  author={Fuglede, Bent and Topsoe, Flemming},
  booktitle={International symposium onInformation theory, 2004. ISIT 2004. Proceedings.},
  pages={31},
  year={2004},
  organization={IEEE}
}

@article{zeng2023huref,
  title={HuRef: HUman-REadable Fingerprint for Large Language Models},
  author={Zeng, Boyi and Zhou, Chenghu and Wang, Xinbing and Lin, Zhouhan},
  journal={arXiv preprint arXiv:2312.04828},
  year={2023}
}

@inproceedings{karras2020analyzing,
  title={Analyzing and improving the image quality of stylegan},
  author={Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8110--8119},
  year={2020}
}

@article{zhang2024reef,
  title={Reef: Representation encoding fingerprints for large language models},
  author={Zhang, Jie and Liu, Dongrui and Qian, Chen and Zhang, Linfeng and Liu, Yong and Qiao, Yu and Shao, Jing},
  journal={arXiv preprint arXiv:2410.14273},
  year={2024}
}

@inproceedings{kornblith2019similarity,
  title={Similarity of neural network representations revisited},
  author={Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  booktitle={International conference on machine learning},
  pages={3519--3529},
  year={2019},
  organization={PMlR}
}

@article{yoon2025intrinsic,
  title={Intrinsic Fingerprint of LLMs: Continue Training is NOT All You Need to Steal A Model!},
  author={Yoon, Do-hyeon and Chun, Minsoo and Allen, Thomas and M{\"u}ller, Hans and Wang, Min and Sharma, Rajesh},
  journal={arXiv preprint arXiv:2507.03014},
  year={2025}
}

@article{jacobs1991adaptive,
  title={Adaptive mixtures of local experts},
  author={Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},
  journal={Neural computation},
  volume={3},
  number={1},
  pages={79--87},
  year={1991},
  publisher={MIT Press}
}

@article{wu2025gradient,
  title={Gradient-Based Model Fingerprinting for LLM Similarity Detection and Family Classification},
  author={Wu, Zehao and Zhao, Yanjie and Wang, Haoyu},
  journal={arXiv preprint arXiv:2506.01631},
  year={2025}
}

@article{song2025riemannian,
  title={Riemannian-Geometric Fingerprints of Generative Models},
  author={Song, Hae Jin and Itti, Laurent},
  journal={arXiv preprint arXiv:2506.22802},
  year={2025}
}

@article{alhazbi2025llms,
  title={LLMs Have Rhythm: Fingerprinting Large Language Models Using Inter-Token Times and Network Traffic Analysis},
  author={Alhazbi, Saeif and Hussain, Ahmed Mohamed and Oligeri, Gabriele and Papadimitratos, Panos},
  journal={arXiv preprint arXiv:2502.20589},
  year={2025}
}
% Sematic Feature as Fingerprint

@misc{yang2024fingerprintlargelanguagemodels,
      title={A Fingerprint for Large Language Models}, 
      author={Zhiguang Yang and Hanzhou Wu},
      year={2024},
      eprint={2407.01235},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2407.01235}, 
}

@article{pasquini2024llmmap,
  title={LLMMap: Fingerprinting for Large Language Models},
  author={Pasquini, Dario and Kornaropoulos, Evgenios M and Ateniese, Giuseppe},
  journal={arXiv preprint arXiv:2407.15847},
  year={2024}
}

@article{sun2024zkllm,
  title={zkLLM: Zero Knowledge Proofs for Large Language Models},
  author={Sun, Haochen and Li, Jason and Zhang, Hongyang},
  journal={arXiv preprint arXiv:2404.16109},
  year={2024}
}

@article{yan2025duffin,
  title={DuFFin: A Dual-Level Fingerprinting Framework for LLMs IP Protection},
  author={Yan, Yuliang and Tang, Haochun and Yan, Shuo and Dai, Enyan},
  journal={arXiv preprint arXiv:2505.16530},
  year={2025}
}

@article{ren2025cotsrf,
  title={CoTSRF: Utilize Chain of Thought as Stealthy and Robust Fingerprint of Large Language Models},
  author={Ren, Zhenzhen and Li, GuoBiao and Li, Sheng and Qian, Zhenxing and Zhang, Xinpeng},
  journal={arXiv preprint arXiv:2505.16785},
  year={2025}
}

@inproceedings{gubri2024trap,
  title={TRAP: Targeted Random Adversarial Prompt Honeypot for Black-Box Identification},
  author={Gubri, Martin and Ulmer, Dennis Thomas and Lee, Hwaran and Yun, Sangdoo and Oh, Seong Joon},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2024},
  pages={11496--11517},
  year={2024}
}

@inproceedings{jin2024proflingo,
    title={Proflingo: A fingerprinting-based intellectual property protection scheme for large language models},
    author={Jin, Heng and Zhang, Chaoyu and Shi, Shanghao and Lou, Wenjing and Hou, Y Thomas},
    booktitle={2024 IEEE Conference on Communications and Network Security (CNS)},
    pages={1--9},
    year={2024},
    organization={IEEE}
}

@misc{xu2025rapsmrobustadversarialprompt,
      title={RAP-SM: Robust Adversarial Prompt via Shadow Models for Copyright Verification of Large Language Models}, 
      author={Zhenhua Xu and Zhebo Wang and Maike Li and Wenpeng Xing and Chunqiang Hu and Chen Zhi and Meng Han},
      year={2025},
      eprint={2505.06304},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2505.06304}, 
}

@article{tsai2025rofl,
    title={RoFL: Robust Fingerprinting of Language Models},
    author={Tsai, Yun-Yun and Guo, Chuan and Yang, Junfeng and van der Maaten, Laurens},
    journal={arXiv preprint arXiv:2505.12682},
    year={2025}
}

@misc{zou2023universal,
    title={Universal and Transferable Adversarial Attacks on Aligned Language Models},
    author={Andy Zou and Zifan Wang and Nicholas Carlini and Milad Nasr and J. Zico Kolter and Matt Fredrikson},
    year={2023},
    eprint={2307.15043},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{bhardwaj2025invisibletracesusinghybrid,
      title={Invisible Traces: Using Hybrid Fingerprinting to identify underlying LLMs in GenAI Apps}, 
      author={Devansh Bhardwaj and Naman Mishra},
      year={2025},
      eprint={2501.18712},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2501.18712}, 
}

@inproceedings{jones2023automatically,
  author    = {E. Jones and others},
  title     = {Automatically Auditing Large Language Models via Discrete Optimization},
  booktitle = {International Conference on Machine Learning, {ICML} 2023, 23-29 July 2023, Honolulu, Hawaii, USA},
  series    = {Proceedings of Machine Learning Research},
  editor    = {A. Krause and others},
  volume    = {202},
  pages     = {15307--15329},
  publisher = {PMLR},
  year      = {2023},
  url       = {https://proceedings.mlr.press/v202/jones23a.html}
}

@article{bitton2025detecting,
  title={Detecting Stylistic Fingerprints of Large Language Models},
  author={Bitton, Yehonatan and Bitton, Elad and Nisan, Shai},
  journal={arXiv preprint arXiv:2503.01659},
  year={2025}
}

@article{ren2025cotsrf,
  title={CoTSRF: Utilize Chain of Thought as Stealthy and Robust Fingerprint of Large Language Models},
  author={Ren, Zhenzhen and Li, GuoBiao and Li, Sheng and Qian, Zhenxing and Zhang, Xinpeng},
  journal={arXiv preprint arXiv:2505.16785},
  year={2025}
}

@article{dasgupta2024watermarking,
  title={Watermarking language models through language models},
  author={Dasgupta, Agnibh and Tanvir, Abdullah and Zhong, Xin},
  journal={arXiv preprint arXiv:2411.05091},
  year={2024}
}
% adversarial examples
@ARTICLE{Cao2019IPGuard,
       author = {{Cao}, Xiaoyu and {Jia}, Jinyuan and {Zhenqiang Gong}, Neil},
        title = "{IPGuard: Protecting Intellectual Property of Deep Neural Networks via Fingerprinting the Classification Boundary}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Cryptography and Security, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
         year = 2019,
        month = oct,
          eid = {arXiv:1910.12903},
        pages = {arXiv:1910.12903},
          doi = {10.48550/arXiv.1910.12903},
archivePrefix = {arXiv},
       eprint = {1910.12903},
 primaryClass = {cs.CR},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv191012903C},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@misc{peng2022fingerprintingdeepneuralnetworks,
      title={Fingerprinting Deep Neural Networks Globally via Universal Adversarial Perturbations}, 
      author={Zirui Peng and Shaofeng Li and Guoxing Chen and Cheng Zhang and Haojin Zhu and Minhui Xue},
      year={2022},
      eprint={2202.08602},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2202.08602}, 
}

@inproceedings{10.5555/3692070.3694306,
author = {Bai, Xiaofan and He, Chaoxiang and Ma, Xiaojing and Zhu, Bin Benjamin and Jin, Hai},
title = {Intersecting-boundary-sensitive fingerprinting for tampering detection of DNN models},
year = {2024},
publisher = {JMLR.org},
abstract = {Cloud-based AI services offer numerous benefits but also introduce vulnerabilities, allowing for tampering with deployed DNN models, ranging from injecting malicious behaviors to reducing computing resources. Fingerprint samples are generated to query models to detect such tampering. In this paper, we present Intersecting-Boundary-Sensitive Fingerprinting (IBSF), a novel method for black-box integrity verification of DNN models using only top-1 labels. Recognizing that tampering with a model alters its decision boundary, IBSF crafts fingerprint samples from normal samples by maximizing the partial Shannon entropy of a selected subset of categories to position the fingerprint samples near decision boundaries where the categories in the subset intersect. These fingerprint samples are almost indistinguishable from their source samples. We theoretically establish and confirm experimentally that these fingerprint samples' expected sensitivity to tampering increases with the cardinality of the subset. Extensive evaluation demonstrates that IBSF surpasses existing state-of-the-art fingerprinting methods, particularly with larger subset cardinality, establishing its state-of-the-art performance in black-box tampering detection using only top-1 labels. The IBSF code is available at: https://github.com/CGCL-codes/IBSF.},
booktitle = {Proceedings of the 41st International Conference on Machine Learning},
articleno = {2236},
numpages = {12},
location = {Vienna, Austria},
series = {ICML'24}
}

@misc{tekgul2023flarefingerprintingdeepreinforcement,
      title={FLARE: Fingerprinting Deep Reinforcement Learning Agents using Universal Adversarial Masks}, 
      author={Buse G. A. Tekgul and N. Asokan},
      year={2023},
      eprint={2307.14751},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2307.14751}, 
}

@inproceedings{ijcai2022p109,
  title     = {MetaFinger: Fingerprinting the Deep Neural Networks with Meta-training},
  author    = {Yang, Kang and Wang, Run and Wang, Lina},
  booktitle = {Proceedings of the Thirty-First International Joint Conference on
               Artificial Intelligence, {IJCAI-22}},
  publisher = {International Joint Conferences on Artificial Intelligence Organization},
  editor    = {Lud De Raedt},
  pages     = {776--782},
  year      = {2022},
  month     = {7},
  note      = {Main Track},
  doi       = {10.24963/ijcai.2022/109},
  url       = {https://doi.org/10.24963/ijcai.2022/109},
}

@inproceedings{NEURIPS2024_804dbf8d,
 author = {Xu, Tianlong and Wang, Chen and Liu, Gaoyang and Yang, Yang and Peng, Kai and Liu, Wei},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {69299--69328},
 publisher = {Curran Associates, Inc.},
 title = {United We Stand, Divided We Fall: Fingerprinting Deep Neural Networks via Adversarial Trajectories},
 url = {https://proceedings.neurips.cc/paper_files/paper/2024/file/804dbf8d3b8eee1ef875c6857efc64eb-Paper-Conference.pdf},
 volume = {37},
 year = {2024}
}

@misc{dathathri2019detectingadversarialexamplesneural,
      title={Detecting Adversarial Examples via Neural Fingerprinting}, 
      author={Sumanth Dathathri and Stephan Zheng and Tianwei Yin and Richard M. Murray and Yisong Yue},
      year={2019},
      eprint={1803.03870},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1803.03870}, 
}

@misc{ren2023ganfingerganbasedfingerprintgeneration,
      title={GanFinger: GAN-Based Fingerprint Generation for Deep Neural Network Ownership Verification}, 
      author={Huali Ren and Anli Yan and Xiaojun Ren and Pei-Gen Ye and Chong-zhi Gao and Zhili Zhou and Jin Li},
      year={2023},
      eprint={2312.15617},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2312.15617}, 
}

@INPROCEEDINGS{10688355,
  author={Gao, ZhenZhe and Tang, Zhenjun and Yin, Zhaoxia and Wu, Baoyuan and Lu, Yue},
  booktitle={2024 IEEE International Conference on Multimedia and Expo (ICME)}, 
  title={Fragile Model Watermark for integrity protection: leveraging boundary volatility and sensitive sample-pairing}, 
  year={2024},
  volume={},
  number={},
  pages={1-6},
  keywords={Sensitivity;Limiting;Neurons;Watermarking;Transforms;Protection;Biological neural networks;DNN Model Watermarking;Sensitive Samples;Backdoor;Fragile Watermarking},
  doi={10.1109/ICME57554.2024.10688355}}


@misc{mickisch2020understandingdecisionboundarydeep,
      title={Understanding the Decision Boundary of Deep Neural Networks: An Empirical Study}, 
      author={David Mickisch and Felix Assion and Florens Greßner and Wiebke Günther and Mariele Motta},
      year={2020},
      eprint={2002.01810},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2002.01810}, 
}

@inproceedings{10.1145/3336191.3372186,
author = {Karimi, Hamid and Tang, Jiliang},
title = {Decision Boundary of Deep Neural Networks: Challenges and Opportunities},
year = {2020},
isbn = {9781450368223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3336191.3372186},
doi = {10.1145/3336191.3372186},
abstract = {One crucial aspect that yet remains fairly unknown while can inform us about the behavior of deep neural networks is their decision boundaries. Trust can be improved once we understand how and why deep models carve out a particular form of decision boundary and thus make particular decisions. Robustness against adversarial examples is directly related to the decision boundary as adversarial examples are basically 'missed out' by the decision boundary between two classes. Investigating the decision boundary of deep neural networks, nevertheless, faces tremendous challenges. First, how we can generate instances near the decision boundary that are similar to real samples? Second, how we can leverage near decision boundary instances to characterize the behaviour of deep neural networks? Motivated to solve these challenges, we focus on investigating the decision boundary of deep neural network classifiers. In particular, we propose a novel approach to generate instances near decision boundary of pre-trained DNNs and then leverage these instances to characterize the behaviour of deep models.},
booktitle = {Proceedings of the 13th International Conference on Web Search and Data Mining},
pages = {919–920},
numpages = {2},
keywords = {robustness, geometrical complexity, deep neural networks, decision boundary, adversarial examples},
location = {Houston, TX, USA},
series = {WSDM '20}
}

@inproceedings{10.5555/3692070.3694306,
author = {Bai, Xiaofan and He, Chaoxiang and Ma, Xiaojing and Zhu, Bin Benjamin and Jin, Hai},
title = {Intersecting-boundary-sensitive fingerprinting for tampering detection of DNN models},
year = {2024},
publisher = {JMLR.org},
abstract = {Cloud-based AI services offer numerous benefits but also introduce vulnerabilities, allowing for tampering with deployed DNN models, ranging from injecting malicious behaviors to reducing computing resources. Fingerprint samples are generated to query models to detect such tampering. In this paper, we present Intersecting-Boundary-Sensitive Fingerprinting (IBSF), a novel method for black-box integrity verification of DNN models using only top-1 labels. Recognizing that tampering with a model alters its decision boundary, IBSF crafts fingerprint samples from normal samples by maximizing the partial Shannon entropy of a selected subset of categories to position the fingerprint samples near decision boundaries where the categories in the subset intersect. These fingerprint samples are almost indistinguishable from their source samples. We theoretically establish and confirm experimentally that these fingerprint samples' expected sensitivity to tampering increases with the cardinality of the subset. Extensive evaluation demonstrates that IBSF surpasses existing state-of-the-art fingerprinting methods, particularly with larger subset cardinality, establishing its state-of-the-art performance in black-box tampering detection using only top-1 labels. The IBSF code is available at: https://github.com/CGCL-codes/IBSF.},
booktitle = {Proceedings of the 41st International Conference on Machine Learning},
articleno = {2236},
numpages = {12},
location = {Vienna, Austria},
series = {ICML'24}
}

@misc{shao2025fitprintfalseclaimresistantmodelownership,
      title={FIT-Print: Towards False-claim-resistant Model Ownership Verification via Targeted Fingerprint}, 
      author={Shuo Shao and Haozhe Zhu and Yiming Li and Hongwei Yao and Tianwei Zhang and Zhan Qin},
      year={2025},
      eprint={2501.15509},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2501.15509}, 
}

% adversarial examples transfer
@article{gu2023survey,
  title={A survey on transferability of adversarial examples across deep neural networks},
  author={Gu, Jindong and Jia, Xiaojun and de Jorge, Pau and Yu, Wenqain and Liu, Xinwei and Ma, Avery and Xun, Yuan and Hu, Anjun and Khakzar, Ashkan and Li, Zhijiang and others},
  journal={arXiv preprint arXiv:2310.17626},
  year={2023}
}
@article{li2025tf,
  title={Tf-attack: Transferable and fast adversarial attacks on large language models},
  author={Li, Zelin and Chen, Kehai and Liu, Lemao and Bai, Xuefeng and Yang, Mingming and Xiang, Yang and Zhang, Min},
  journal={Knowledge-Based Systems},
  volume={312},
  pages={113117},
  year={2025},
  publisher={Elsevier}
}

% weight watermark as fingerprint
@inproceedings{uchida2017embedding,
  title={Embedding watermarks into deep neural networks},
  author={Uchida, Yusuke and Nagai, Yuki and Sakazawa, Shigeyuki and Satoh, Shin'ichi},
  booktitle={Proceedings of the 2017 ACM on international conference on multimedia retrieval},
  pages={269--277},
  year={2017}
}

@inproceedings{rouhani2019deepsigns,
  title={DeepSigns: An End-to-End Watermarking Framework for Ownership Protection of Deep Neural Networks},
  author={Rouhani, Bita Darvish and Chen, Huili and Koushanfar, Farinaz},
  booktitle={Proceedings of the Twenty‑Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS) 2019},
  pages={485--497},
  year={2019}
}

@inproceedings{kuribayashi2021immunization,
  title={Immunization of Pruning Attack in DNN Watermarking Using Constant Weight Code},
  author={Kuribayashi, Minoru and Yasui, Tatsuya and Malik, Asad and Funabiki, Nobuo},
  booktitle={Proceedings of ICASSP 2023 (arXiv preprint arXiv:2107.02961)},
  pages={1--5},
  year={2023}
}

@article{tondi2022robust,
  title={Robust and Large‑Payload DNN Watermarking via Fixed, Distribution‑Optimized, Weights},
  author={Tondi, Benedetta and Costanzo, Andrea and Barni, Mauro},
  journal={IEEE Transactions on Dependable and Secure Computing},
  year={2024},
  eprint={2208.10973}
}

@inproceedings{zhang2024emmark,
  title={EmMark: Robust watermarks for IP protection of embedded quantized large language models},
  author={Zhang, Ruisi and Koushanfar, Farinaz},
  booktitle={Proceedings of the 61st ACM/IEEE Design Automation Conference},
  pages={1--6},
  year={2024}
}

@article{guo2025invariant,
    title={Invariant‑Based Robust Weights Watermark for Large Language Models},
    author={Guo, Qingxiao and Zhu, Xinjie and Ma, Yilong and Jin, Hui and Wang, Yunhao and Zhang, Weifeng and Guo, Xiaobing},
    journal={arXiv preprint arXiv:2507.08288},
    year={2025}
  }

@article{block2025robust,
    title={Robust and Efficient Watermarking of Large Language Models Using Error Correction Codes},
    author={Block, Adam and Sekhari, Ayush and Rakhlin, Alexander},
    journal={Proceedings on Privacy Enhancing Technologies (PoPETs) 2025},
    year={2025}
  }

@inproceedings{fernandez2023functional,
  title={Functional Invariants to Watermark Large Transformers},
  author={Fernandez, Pierre and Couairon, Guillaume and Furon, Teddy and Douze, Matthijs},
  booktitle={ICASSP 2023},
  year={2023}
}

% backdoor watermark as fingerprint
@inproceedings{xu2024instructional,
  title={Instructional Fingerprinting of Large Language Models},
  author={Xu, Jiashu and Wang, Fei and Ma, Mingyu and Koh, Pang Wei and Xiao, Chaowei and Chen, Muhao},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={3277--3306},
  year={2024}
}

@inproceedings{zhang2025scalable,
  title={Scalable Fingerprinting of Large Language Models},
  author={Zhang, Jie and Liu, Dongrui and Qian, Chen and Zhang, Linfeng and Liu, Yong and Qiao, Yu and Shao, Jing},
  booktitle={International Conference on Learning Representations},
  year={2025}
}

@article{zhang2025imf,
  title={ImF: Implicit Fingerprint for Large Language Models},
  author={Zhang, Jie and Liu, Dongrui and Qian, Chen and Zhang, Linfeng and Liu, Yong and Qiao, Yu and Shao, Jing},
  journal={arXiv preprint arXiv:2503.21805},
  year={2025}
}

@article{cai2024utf,
  title={UTF: Undertrained Tokens as Fingerprints A Novel Approach to LLM Identification},
  author={Cai, Jiacheng and Yu, Jiahao and Shao, Yangguang and Wu, Yuhang and Xing, Xinyu},
  journal={arXiv preprint arXiv:2410.12318},
  year={2024}
}

@article{russinovich2024hey,
  title={Hey, That's My Model! Introducing Chain \& Hash, An LLM Fingerprinting Technique},
  author={Russinovich, Mark and Salem, Ahmed},
  journal={arXiv preprint arXiv:2407.10887},
  year={2024}
}


@inproceedings{yamabe-etal-2025-mergeprint,
    title = "{M}erge{P}rint: Merge-Resistant Fingerprints for Robust Black-box Ownership Verification of Large Language Models",
    author = "Yamabe, Shojiro  and
      Waseda, Futa Kai  and
      Takahashi, Tsubasa  and
      Wataoka, Koki",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.acl-long.342/",
    doi = "10.18653/v1/2025.acl-long.342",
    pages = "6894--6916",
    ISBN = "979-8-89176-251-0",
    abstract = "Protecting the intellectual property of Large Language Models (LLMs) has become increasingly critical due to the high cost of training. Model merging, which integrates multiple expert models into a single multi-task model, introduces a novel risk of unauthorized use of LLMs due to its efficient merging process. While fingerprinting techniques have been proposed for verifying model ownership, their resistance to model merging remains unexplored. To address this gap, we propose a novel fingerprinting method, MergePrint, which embeds robust fingerprints capable of surviving model merging. MergePrint enables black-box ownership verification, where owners only need to check if a model produces target outputs for specific fingerprint inputs, without accessing model weights or intermediate outputs. By optimizing against a pseudo-merged model that simulates merged behavior, MergePrint ensures fingerprints that remain detectable after merging. Additionally, to minimize performance degradation, we pre-optimize the fingerprint inputs. MergePrint pioneers a practical solution for black-box ownership verification, protecting LLMs from misappropriation via merging, while also excelling in resistance to broader model theft threats."
}

@inproceedings{10.1145/3240765.3240862,
author = {Guo, Jia and Potkonjak, Miodrag},
title = {Watermarking deep neural networks for embedded systems},
year = {2018},
isbn = {9781450359504},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3240765.3240862},
doi = {10.1145/3240765.3240862},
abstract = {Deep neural networks (DNNs) have become an important tool for bringing intelligence to mobile and embedded devices. The increasingly wide deployment, sharing and potential commercialization of DNN models create a compelling need for intellectual property (IP) protection. Recently, DNN watermarking emerges as a plausible IP protection method. Enabling DNN watermarking on embedded devices in a practical setting requires a black-box approach. Existing DNN watermarking frameworks either fail to meet the black-box requirement or are susceptible to several forms of attacks. We propose a watermarking framework by incorporating the author's signature in the process of training DNNs. While functioning normally in regular cases, the resulting watermarked DNN behaves in a different, predefined pattern when given any signed inputs, thus proving the authorship. We demonstrate an example implementation of the framework on popular image classification datasets and show that strong watermarks can be embedded in the models.},
booktitle = {Proceedings of the International Conference on Computer-Aided Design},
articleno = {133},
numpages = {8},
keywords = {deep neural networks, embedded systems, watermarking},
location = {San Diego, California},
series = {ICCAD '18}
}


@article{Gu2022WatermarkingPL,
  title={Watermarking Pre-trained Language Models with Backdooring},
  author={Chenxi Gu and Chengsong Huang and Xiaoqing Zheng and Kai-Wei Chang and Cho-Jui Hsieh},
  journal={ArXiv},
  year={2022},
  volume={abs/2210.07543},
  url={https://api.semanticscholar.org/CorpusID:252907247}
}

@inproceedings{10.5555/3600270.3601232,
author = {Li, Yiming and Bai, Yang and Jiang, Yong and Yang, Yong and Xia, Shu-Tao and Li, Bo},
title = {Untargeted backdoor watermark: towards harmless and stealthy dataset copyright protection},
year = {2022},
isbn = {9781713871088},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Deep neural networks (DNNs) have demonstrated their superiority in practice. Arguably, the rapid development of DNNs is largely benefited from high-quality (open-sourced) datasets, based on which researchers and developers can easily evaluate and improve their learning methods. Since the data collection is usually time-consuming or even expensive, how to protect their copyrights is of great significance and worth further exploration. In this paper, we revisit dataset ownership verification. We find that existing verification methods introduced new security risks in DNNs trained on the protected dataset, due to the targeted nature of poison-only backdoor watermarks. To alleviate this problem, in this work, we explore the untargeted backdoor watermarking scheme, where the abnormal model behaviors are not deterministic. Specifically, we introduce two dispersibilities and prove their correlation, based on which we design the untargeted backdoor watermark under both poisoned-label and clean-label settings. We also discuss how to use the proposed untargeted backdoor watermark for dataset ownership verification. Experiments on benchmark datasets verify the effectiveness of our methods and their resistance to existing backdoor defenses.},
booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
articleno = {962},
numpages = {13},
location = {New Orleans, LA, USA},
series = {NIPS '22}
}

% weight quantization
@inproceedings{li2023watermarking,
  title={Watermarking LLMs with Weight Quantization},
  author={Li, Linyang and Jiang, Bo and Wang, Peng and Ren, Kui and Yan, Haoran and Qiu, Xipeng},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={3229--3243},
  year={2023}
}

@misc{openai2024gpt4technicalreport,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and others},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{geminiteam2025geminifamilyhighlycapable,
      title={Gemini: A Family of Highly Capable Multimodal Models}, 
      author={Gemini Team and Rohan Anil and Sebastian Borgeaud and Jean-Baptiste Alayrac and Jiahui Yu and Radu Soricut and Johan Schalkwyk and Andrew M. Dai and Anja Hauth and Katie Millican and David Silver and Melvin Johnson and Ioannis Antonoglou and Julian Schrittwieser and Amelia Glaese and Jilin Chen and Emily Pitler and Timothy Lillicrap and Angeliki Lazaridou and Orhan Firat and James Molloy and Michael Isard and Paul R. Barham and Tom Hennigan and Benjamin Lee and Fabio Viola and Malcolm Reynolds and Yuanzhong Xu and Ryan Doherty and Eli Collins and Clemens Meyer and Eliza Rutherford and Erica Moreira and Kareem Ayoub and Megha Goel and others},
      year={2025},
      eprint={2312.11805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2312.11805}, 
}

@misc{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and others},
      year={2025},
      eprint={2501.12948},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.12948}, 
}

@misc{deepseekai2025deepseekv3technicalreport,
      title={DeepSeek-V3 Technical Report}, 
      author={DeepSeek-AI and Aixin Liu and Bei Feng and Bing Xue and Bingxuan Wang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and others},
      year={2025},
      eprint={2412.19437},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.19437}, 
}

@misc{claude2024,
  author = {Anthropic},
  title = {Claude},
  year = {2025},
  url = {https://claude.ai/},
  note = {Accessed: 2025},
  publisher = {Anthropic}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@article{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  journal={See https://vicuna. lmsys. org (accessed 14 April 2023)},
  year={2023}
}

@article{yang2024qwen2,
  title={Qwen2. 5 technical report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}

@article{luo2023wizardmath,
  title={Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct},
  author={Luo, Haipeng and Sun, Qingfeng and Xu, Can and Zhao, Pu and Lou, Jianguang and Tao, Chongyang and Geng, Xiubo and Lin, Qingwei and Chen, Shifeng and Zhang, Dongmei},
  journal={arXiv preprint arXiv:2308.09583},
  year={2023}
}

@software{together2023redpajama,
  author = {Together Computer},
  title = {RedPajama: An Open Source Recipe to Reproduce LLaMA training dataset},
  month = April,
  year = 2023,
  url = {https://github.com/togethercomputer/RedPajama-Data}
}

@misc{llama3herd,
  author = {Shenghao and Zha, Shengxin Cindy and Shankar, Shiva and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Kohler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook V andontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Albiero, Vítor and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wang, Xiaofang and Wu, Xiaojian and Wang, Xiaolan and Xia, Xide and Wu, Xilun and Gao, Xinbo and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and Wang and Hao, Yuchen and Qian, Yundi and He, Yuzi and Rait, Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei},
  title = {The Llama 3 Herd of Models},
  year = {2024},
  eprint = {2407.21783},
  primaryclass = {cs.AI},
  url = {https://arxiv.org/abs/2407.21783},
  keywords = {llama3},
  timestamp = {2024-11-15T16:37:56.000+0100},
  biburl = {https://www.bibsonomy.org/bibtex/2c985c5d9b0752a1f1951e9ca6375c0f5/albinzehe}
}


% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkh{\"a}user" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

%Entries

%introduction
@article{rawat2017deep,
  title={Deep convolutional neural networks for image classification: A comprehensive review},
  author={Rawat, Waseem and Wang, Zenghui},
  journal={Neural computation},
  volume={29},
  number={9},
  pages={2352--2449},
  year={2017},
  publisher={MIT Press}
}

@book{medsker1999recurrent,
  title={Recurrent neural networks: design and applications},
  author={Medsker, Larry and Jain, Lakhmi C},
  year={1999},
  publisher={CRC press}
}

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron and Bengio, Yoshua},
  volume={1},
  number={2},
  year={2016},
  publisher={MIT press Cambridge}
}

@article{lecun2015deep,
  title={Deep learning},
  author={LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  journal={nature},
  volume={521},
  number={7553},
  pages={436--444},
  year={2015},
  publisher={Nature Publishing Group UK London}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@misc{xie2025memorizationlargelanguagemodels,
      title={On Memorization of Large Language Models in Logical Reasoning}, 
      author={Chulin Xie and Yangsibo Huang and Chiyuan Zhang and Da Yu and Xinyun Chen and Bill Yuchen Lin and Bo Li and Badih Ghazi and Ravi Kumar},
      year={2025},
      eprint={2410.23123},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.23123}, 
}

@misc{xie2025logicrlunleashingllmreasoning,
      title={Logic-RL: Unleashing LLM Reasoning with Rule-Based Reinforcement Learning}, 
      author={Tian Xie and Zitian Gao and Qingnan Ren and Haoming Luo and Yuqian Hong and Bryan Dai and Joey Zhou and Kai Qiu and Zhirong Wu and Chong Luo},
      year={2025},
      eprint={2502.14768},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.14768}, 
}

@misc{surana2025structuredprogramsynthesisusing,
      title={Structured Program Synthesis using LLMs: Results and Insights from the IPARC Challenge}, 
      author={Shraddha Surana and Ashwin Srinivasan and Michael Bain},
      year={2025},
      eprint={2506.13820},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2506.13820}, 
}

@misc{bang2023multitaskmultilingualmultimodalevaluation,
      title={A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity}, 
      author={Yejin Bang and Samuel Cahyawijaya and Nayeon Lee and Wenliang Dai and Dan Su and Bryan Wilie and Holy Lovenia and Ziwei Ji and Tiezheng Yu and Willy Chung and Quyet V. Do and Yan Xu and Pascale Fung},
      year={2023},
      eprint={2302.04023},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2302.04023}, 
}

@misc{jiao2023chatgptgoodtranslatoryes,
      title={Is ChatGPT A Good Translator? Yes With GPT-4 As The Engine}, 
      author={Wenxiang Jiao and Wenxuan Wang and Jen-tse Huang and Xing Wang and Shuming Shi and Zhaopeng Tu},
      year={2023},
      eprint={2301.08745},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2301.08745}, 
}

@misc{duan2025measuringscientificcapabilitieslanguage,
      title={Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab}, 
      author={Haonan Duan and Stephen Zhewen Lu and Caitlin Fiona Harrigan and Nishkrit Desai and Jiarui Lu and Michał Koziarski and Leonardo Cotta and Chris J. Maddison},
      year={2025},
      eprint={2507.02083},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2507.02083}, 
}

@misc{chen2025ai4researchsurveyartificialintelligence,
      title={AI4Research: A Survey of Artificial Intelligence for Scientific Research}, 
      author={Qiguang Chen and Mingda Yang and Libo Qin and Jinhao Liu and Zheng Yan and Jiannan Guan and Dengyun Peng and Yiyan Ji and Hanjing Li and Mengkang Hu and Yimeng Zhang and Yihao Liang and Yuhang Zhou and Jiaqi Wang and Zhi Chen and Wanxiang Che},
      year={2025},
      eprint={2507.01903},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2507.01903}, 
}

@misc{aly2025evaluationlargelanguagemodels,
      title={An Evaluation of Large Language Models on Text Summarization Tasks Using Prompt Engineering Techniques}, 
      author={Walid Mohamed Aly and Taysir Hassan A. Soliman and Amr Mohamed AbdelAziz},
      year={2025},
      eprint={2507.05123},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2507.05123}, 
}

@INPROCEEDINGS{7944061,
  author={Moratanch, N. and Chitrakala, S.},
  booktitle={2017 International Conference on Computer, Communication and Signal Processing (ICCCSP)}, 
  title={A survey on extractive text summarization}, 
  year={2017},
  volume={},
  number={},
  pages={1-6},
  keywords={Feature extraction;Unsupervised learning;Supervised learning;Fuzzy logic;Data mining;Encyclopedias;Text Summarization;Unsupervised Learning;Supervised Learning;Sentence Fusion;Extraction Scheme;Sentence Revision;Extractive Summary},
  doi={10.1109/ICCCSP.2017.7944061}}

@inproceedings{PetitBikim2024,
  author    = {Jean Petit Bikim and Carick Appolinaire Atezong Ymele and Azanzi Jiomekong and Allard Oelen and Gollam Rabby and Jennifer D'Souza and S{\"o}ren Auer},
  title     = {Leveraging GPT Models For Semantic Table Annotation},
  booktitle = {SemTab@ISWC (CEUR Workshop Proceedings, Vol. 3889)},
  year      = {2024},
  pages     = {43--53}
}

@inproceedings{Dasoulas2023,
  author    = {Ioannis Dasoulas and Duo Yang and Xuemin Duan and Anastasia Dimou},
  title     = {TorchicTab: Semantic Table Annotation with Wikidata and Language Models},
  booktitle = {SemTab@ISWC (CEUR Workshop Proceedings, Vol. 3557)},
  year      = {2023},
  pages     = {21--37}
}

@inproceedings{Huynh2022,
  author    = {Viet-Phi Huynh and Yoan Chabot and Thomas Labb{\'e} and Jixiong Liu and Rapha{\"e}l Troncy},
  title     = {From Heuristics to Language Models: A Journey Through the Universe of Semantic Table Interpretation with {DAGOBAH}},
  booktitle = {SemTab@ISWC (CEUR Workshop Proceedings, Vol. 3320)},
  year      = {2022},
  pages     = {45--58}
}

@misc{koletsis2025relationshipdetectiontabulardata,
      title={Relationship Detection on Tabular Data Using Statistical Analysis and Large Language Models}, 
      author={Panagiotis Koletsis and Christos Panagiotopoulos and Georgios Th. Papadopoulos and Vasilis Efthymiou},
      year={2025},
      eprint={2506.06371},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2506.06371}, 
}

@misc{creativecommons_licenses,
  author = {{Creative Commons}},
  title = {Licenses},
  year = {2025},
  url = {https://creativecommons.org/share-your-work/cclicenses/},
  note = {Accessed: 2025},
}

@misc{ApacheLicense2.0,
  author = {{Apache Software Foundation}},
  title = {Apache License, Version 2.0},
  year = {2025},
  month = jan,
  url = {https://www.apache.org/licenses/LICENSE-2.0},
  note = {Accessed: 2025},
}

%introduction-other-watermarking-surveys
-----------
@article{alkawaz2016concise,
  title={Concise analysis of current text automation and watermarking approaches},
  author={Alkawaz, Mohammed Hazim and Sulong, Ghazali and Saba, Tanzila and Almazyad, Abdulaziz S and Rehman, Amjad},
  journal={Security and Communication Networks},
  volume={9},
  number={18},
  pages={6365--6378},
  year={2016},
  publisher={Wiley Online Library}
}

@article{kamaruddin2018review,
  title={A review of text watermarking: theory, methods, and applications},
  author={Kamaruddin, Nurul Shamimi and Kamsin, Amirrudin and Por, Lip Yee and Rahman, Hameedur},
  journal={IEEE Access},
  volume={6},
  pages={8011--8028},
  year={2018},
  publisher={IEEE}
}

@article{boenisch2021systematic,
  title={A systematic review on model watermarking for neural networks},
  author={Boenisch, Franziska},
  journal={Frontiers in big Data},
  volume={4},
  pages={729663},
  year={2021},
  publisher={Frontiers Media SA}
}

@article{lederer2023identifying,
  title={Identifying appropriate intellectual property protection mechanisms for machine learning models: a systematization of watermarking, fingerprinting, model access, and attacks},
  author={Lederer, Isabell and Mayer, Rudolf and Rauber, Andreas},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  year={2023},
  publisher={IEEE}
}

@inproceedings{hwang2023brief,
  title={A brief survey of watermarks in generative AI},
  author={Hwang, JaeYoung and Oh, SangHoon},
  booktitle={2023 14th International Conference on Information and Communication Technology Convergence (ICTC)},
  pages={1157--1160},
  year={2023},
  organization={IEEE}
}

@article{liu2024survey,
  title={A survey of text watermarking in the era of large language models},
  author={Liu, Aiwei and Pan, Leyi and Lu, Yijian and Li, Jingjing and Hu, Xuming and Zhang, Xi and Wen, Lijie and King, Irwin and Xiong, Hui and Yu, Philip},
  journal={ACM Computing Surveys},
  volume={57},
  number={2},
  pages={1--36},
  year={2024},
  publisher={ACM New York, NY}
}

@article{lalai2024intentions,
  title={From intentions to techniques: A comprehensive taxonomy and challenges in text watermarking for large language models},
  author={Lalai, Harsh Nishant and Ramakrishnan, Aashish Anantha and Shah, Raj Sanjay and Lee, Dongwon},
  journal={arXiv preprint arXiv:2406.11106},
  year={2024}
}

@article{wang2025building,
  title={Building intelligence identification system via large language model watermarking: a survey and beyond},
  author={Wang, Xuhong and Jiang, Haoyu and Yu, Yi and Yu, Jingru and Lin, Yilun and Yi, Ping and Wang, Yingchun and Qiao, Yu and Li, Li and Wang, Fei-Yue},
  journal={Artificial Intelligence Review},
  volume={58},
  number={8},
  pages={249},
  year={2025},
  publisher={Springer}
}

@article{zhang2024watermarking,
  title={Watermarking Large Language Models and the Generated Content: Opportunities and Challenges},
  author={Zhang, Ruisi and Koushanfar, Farinaz},
  journal={arXiv preprint arXiv:2410.19096},
  year={2024}
}

@article{yang2025watermarking,
  title={Watermarking for large language models: A survey},
  author={Yang, Zhiguang and Zhao, Gejian and Wu, Hanzhou},
  journal={Mathematics},
  volume={13},
  number={9},
  pages={1420},
  year={2025},
  publisher={MDPI}
}

@article{liang2024watermarking,
  title={Watermarking techniques for large language models: A survey},
  author={Liang, Yuqing and Xiao, Jiancheng and Gan, Wensheng and Yu, Philip S},
  journal={arXiv preprint arXiv:2409.00089},
  year={2024}
}
-----------
%agent-survey
-----------
@misc{kong2025surveyllmdrivenaiagent,
      title={A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures}, 
      author={Dezhang Kong and Shi Lin and Zhenhua Xu and Zhebo Wang and Minghao Li and Yufeng Li and Yilun Zhang and Hujin Peng and Zeyang Sha and Yuyuan Li and Changting Lin and Xun Wang and Xuan Liu and Ningyu Zhang and Chaochao Chen and Muhammad Khurram Khan and Meng Han},
      year={2025},
      eprint={2506.19676},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2506.19676}, 

-----------
%model-merging
-----------
@article{arora2024here,
  title={Here's a Free Lunch: Sanitizing Backdoored Models with Model Merge},
  author={Arora, Ansh and He, Xuanli and Mozes, Maximilian and Swain, Srinibas and Dras, Mark and Xu, Qiongkai},
  journal={arXiv preprint arXiv:2402.19334},
  year={2024}
}

@article{bhardwaj2024language,
  title={Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic},
  author={Bhardwaj, Rishabh and Anh, Do Duc and Poria, Soujanya},
  journal={arXiv preprint arXiv:2402.11746},
  year={2024}
}

@inproceedings{goddard-etal-2024-mergekit,
    title = "Arcee{'}s {M}erge{K}it: A Toolkit for Merging Large Language Models",
    author = "Goddard, Charles  and
      Siriwardhana, Shamane  and
      Ehghaghi, Malikeh  and
      Meyers, Luke  and
      Karpukhin, Vladimir  and
      Benedict, Brian  and
      McQuade, Mark  and
      Solawetz, Jacob",
    editor = "Dernoncourt, Franck  and
      Preo{\c{t}}iuc-Pietro, Daniel  and
      Shimorina, Anastasia",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track",
    month = nov,
    year = "2024",
    address = "Miami, Florida, US",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-industry.36",
    doi = "10.18653/v1/2024.emnlp-industry.36",
    pages = "477--485",
    abstract = "The rapid growth of open-source language models provides the opportunity to merge model checkpoints, combining their parameters to improve performance and versatility. Advances in transfer learning have led to numerous task-specific models, which model merging can integrate into powerful multitask models without additional training. MergeKit is an open-source library designed to support this process with an efficient and extensible framework suitable for any hardware. It has facilitated the merging of thousands of models, contributing to some of the world{'}s most powerful open-source model checkpoints. The library is accessible at: https://github.com/arcee-ai/mergekit.",
}


@article{cong2024have,
  title={Have You Merged My Model? On The Robustness of Large Language Model IP Protection Methods Against Model Merging},
  author={Cong, Tianshuo and Ran, Delong and Liu, Zesen and He, Xinlei and Liu, Jinyuan and Gong, Yichen and Li, Qi and Wang, Anyu and Wang, Xiaoyun},
  journal={arXiv preprint arXiv:2404.05188},
  year={2024}
}

@article{ilharco2022task-arithmetic,
  title={Editing models with task arithmetic},
  author={Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Gururangan, Suchin and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali},
  journal={arXiv preprint arXiv:2212.04089},
  year={2022}
}

@article{yadav2024ties,
  title={Ties-merging: Resolving interference when merging models},
  author={Yadav, Prateek and Tam, Derek and Choshen, Leshem and Raffel, Colin A and Bansal, Mohit},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{yu2024dare,
  title={Language models are super mario: Absorbing abilities from homologous models as a free lunch},
  author={Yu, Le and Yu, Bowen and Yu, Haiyang and Huang, Fei and Li, Yongbin},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}

-----------
%training-strategies
-----------

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}
-----------
%evaluation
-----------
@article{xu2025mark,
  title={Mark your llm: Detecting the misuse of open-source large language models via watermarking},
  author={Xu, Yijie and Liu, Aiwei and Hu, Xuming and Wen, Lijie and Xiong, Hui},
  journal={arXiv preprint arXiv:2503.04636},
  year={2025}
}

@article{nasery2025scalable,
  title={Scalable fingerprinting of large language models},
  author={Nasery, Anshul and Hayase, Jonathan and Brooks, Creston and Sheng, Peiyao and Tyagi, Himanshu and Viswanath, Pramod and Oh, Sewoong},
  journal={arXiv preprint arXiv:2502.07760},
  year={2025}
}

@article{wu2025imf,
  title={ImF: Implicit Fingerprint for Large Language Models},
  author={Wu, Jiaxuan and Peng, Wanli and Xue, Yiming and Wen, Juan and others},
  journal={CoRR},
  year={2025}
}

@inproceedings{li2023plmmark,
  title={PLMmark: A Secure and Robust Black-Box Watermarking Framework for Pre-trained Language Models},
  author={Li, Peixuan and Cheng, Pengzhou and Li, Fangqi and Du, Wei and Zhao, Haodong and Liu, Gongshen},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence 2023},
  pages={14991--14999},
  year={2023}
}

@article{li2024double,
  title={Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning},
  author={Li, Shen and Yao, Liuyi and Gao, Jinyang and Zhang, Lan and Li, Yaliang},
  journal={arXiv preprint arXiv:2402.14883},
  year={2024}
}

@article{Le_Merrer_2019,
   title={Adversarial frontier stitching for remote neural network watermarking},
   volume={32},
   ISSN={1433-3058},
   url={http://dx.doi.org/10.1007/s00521-019-04434-z},
   DOI={10.1007/s00521-019-04434-z},
   number={13},
   journal={Neural Computing and Applications},
   publisher={Springer Science and Business Media LLC},
   author={Le Merrer, Erwan and Pérez, Patrick and Trédan, Gilles},
   year={2019},
   month=aug, pages={9233–9244} }


@inproceedings{zhang2018protecting,
  title={Protecting intellectual property of deep neural networks with watermarking},
  author={Zhang, Jialong and Gu, Zhongshu and Jang, Jiyong and Wu, Hui and Stoecklin, Marc Ph and Huang, Heqing and Molloy, Ian},
  booktitle={Proceedings of the 2018 on Asia conference on computer and communications security},
  pages={159--172},
  year={2018}
}

@inproceedings{adi2018turning,
  title={Turning your weakness into a strength: Watermarking deep neural networks by backdooring},
  author={Adi, Yossi and Baum, Carsten and Cisse, Moustapha and Pinkas, Benny and Keshet, Joseph},
  booktitle={27th USENIX security symposium (USENIX Security 18)},
  pages={1615--1631},
  year={2018}
}

@article{rouhani2018deepsigns,
  title={Deepsigns: A generic watermarking framework for ip protection of deep learning models},
  author={Rouhani, Bita Darvish and Chen, Huili and Koushanfar, Farinaz},
  journal={arXiv preprint arXiv:1804.00750},
  year={2018}
}

@inproceedings{wang2021riga,
  title={Riga: Covert and robust white-box watermarking of deep neural networks},
  author={Wang, Tianhao and Kerschbaum, Florian},
  booktitle={Proceedings of the web conference 2021},
  pages={993--1004},
  year={2021}
}

@inproceedings{tekgul2021waffle,
  title={Waffle: Watermarking in federated learning},
  author={Tekgul, Buse GA and Xia, Yuxi and Marchal, Samuel and Asokan, N},
  booktitle={2021 40th International Symposium on Reliable Distributed Systems (SRDS)},
  pages={310--320},
  year={2021},
  organization={IEEE}
}

@article{fan2021deepipr,
  title={Deepipr: Deep neural network ownership verification with passports},
  author={Fan, Lixin and Ng, Kam Woh and Chan, Chee Seng and Yang, Qiang},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={44},
  number={10},
  pages={6122--6139},
  year={2021},
  publisher={IEEE}
}
-----------
%training strategies
-----------

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}
-----------
% pruning
-----------
@inproceedings{ma2023llmpruner,
  title={LLM-Pruner: On the Structural Pruning of Large Language Models},
  author={Xinyin Ma and Gongfan Fang and Xinchao Wang},
  booktitle={Advances in Neural Information Processing Systems},
  year={2023},
}


-----------
% agent error propagation
-----------
@misc{wang2025gsafeguardtopologyguidedsecuritylens,
      title={G-Safeguard: A Topology-Guided Security Lens and Treatment on LLM-based Multi-agent Systems}, 
      author={Shilong Wang and Guibin Zhang and Miao Yu and Guancheng Wan and Fanci Meng and Chongye Guo and Kun Wang and Yang Wang},
      year={2025},
      eprint={2502.11127},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2502.11127}, 
}

@misc{yu2024netsafeexploringtopologicalsafety,
      title={NetSafe: Exploring the Topological Safety of Multi-agent Networks}, 
      author={Miao Yu and Shilong Wang and Guibin Zhang and Junyuan Mao and Chenlong Yin and Qijiong Liu and Qingsong Wen and Kun Wang and Yang Wang},
      year={2024},
      eprint={2410.15686},
      archivePrefix={arXiv},
      primaryClass={cs.MA},
      url={https://arxiv.org/abs/2410.15686}, 
}

@misc{zhou2025corbacontagiousrecursiveblocking,
      title={CORBA: Contagious Recursive Blocking Attacks on Multi-Agent Systems Based on Large Language Models}, 
      author={Zhenhong Zhou and Zherui Li and Jie Zhang and Yuanhe Zhang and Kun Wang and Yang Liu and Qing Guo},
      year={2025},
      eprint={2502.14529},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.14529}, 
}

% Format-Based Watermarking
@article{brassil1995electronic,
  title={Electronic marking and identification techniques to discourage document copying},
  author={Brassil, Jack T and Low, Steven and Maxemchuk, Nicholas F. and O'Gorman, Lawrence},
  journal={IEEE Journal on Selected Areas in Communications},
  volume={13},
  number={8},
  pages={1495--1504},
  year={1995},
  publisher={IEEE}
}


@article{POR20121075,
    title = {UniSpaCh: A text-based data hiding method using Unicode space characters},
    journal = {Journal of Systems and Software},
    volume = {85},
    number = {5},
    pages = {1075-1082},
    year = {2012},
    issn = {0164-1212},
    doi = {https://doi.org/10.1016/j.jss.2011.12.023},
    url = {https://www.sciencedirect.com/science/article/pii/S0164121211003177},
    author = {Lip Yee Por and KokSheik Wong and Kok Onn Chee},
    keywords = {UniSpaCh, DASH, Data hiding, Unicode character, Space manipulation},
    abstract = {This paper proposes a text-based data hiding method to insert external information into Microsoft Word document. First, the drawback of low embedding efficiency in the existing text-based data hiding methods is addressed, and a simple attack, DASH, is proposed to reveal the information inserted by the existing text-based data hiding methods. Then, a new data hiding method, UniSpaCh, is proposed to counter DASH. The characteristics of Unicode space characters with respect to embedding efficiency and DASH are analyzed, and the selected Unicode space characters are inserted into inter-sentence, inter-word, end-of-line and inter-paragraph spacings to encode external information while improving embedding efficiency and imperceptivity of the embedded information. UniSpaCh is also reversible where the embedded information can be removed to completely reconstruct the original Microsoft Word document. Experiments were carried out to verify the performance of UniSpaCh as well as comparing it to the existing space-manipulating data hiding methods. Results suggest that UniSpaCh offers higher embedding efficiency while exhibiting higher imperceptivity of white space manipulation when compared to the existing methods considered. In the best case scenario, UniSpaCh produces output document of size almost 9 times smaller than that of the existing method.}
}

@inproceedings{rizzo2016content,
  title={Content-preserving text watermarking through unicode homoglyph substitution},
  author={Rizzo, Stefano Giovanni and Bertini, Flavio and Montesi, Danilo},
  booktitle={Proceedings of the 20th International Database Engineering \& Applications Symposium},
  year={2016}
}

@article{sato2023embarrassingly,
  title={Embarrassingly Simple Text Watermarks},
  author={Sato, Ryoma and Takezawa, Yuki and Bao, Han and Niwa, Kenta and Yamada, Makoto},
  journal={arXiv preprint arXiv:2310.08920},
  year={2023}
}

% Synonym-Based Watermarking

@inproceedings{topkara2006hiding,
  title={The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions},
  author={Topkara, Umut and Topkara, Mercan and Atallah, Mikhail J},
  booktitle={Proceedings of the 8th workshop on Multimedia and security},
  pages={164--174},
  year={2006}
}

@article{munyer2023deeptextmark,
  title={Deeptextmark: Deep learning based text watermarking for detection of large language model generated text},
  author={Munyer, Travis and Zhong, Xin},
  journal={arXiv preprint arXiv:2305.05773},
  year={2023}
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas},
  journal={arXiv preprint arXiv:1301.3781},
  volume={3781},
  year={2013}
}

@inproceedings{cer2018universal,
  title={Universal sentence encoder for English},
  author={Cer, Daniel and Yang, Yinfei and Kong, Sheng-yi and Hua, Nan and Limtiaco, Nicole and John, Rhomni St and Constant, Noah and Guajardo-Cespedes, Mario and Yuan, Steve and Tar, Chris and others},
  booktitle={Proceedings of the 2018 conference on empirical methods in natural language processing: system demonstrations},
  pages={169--174},
  year={2018}
}

@article{yang2023watermarking,
  title={Watermarking Text Generated by Black-Box Language Models},
  author={Yang, Xi and Chen, Kejiang and Zhang, Weiming and Liu, Chang and Qi, Yuang and Zhang, Jie and Fang, Han and Yu, Nenghai},
  journal={arXiv preprint arXiv:2305.08883},
  year={2023}
}

@inproceedings{yoo2023robust,
  title={Robust multi-bit natural language watermarking through invariant features},
  author={Yoo, KiYoon and Ahn, Wonhyuk and Jang, Jiho and Kwak, Nojun},
  booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={2092--2115},
  year={2023}
}

@misc{devlin2019bertpretrainingdeepbidirectional,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}


% Syntactic-Based Watermarking
@inproceedings{atallah2001natural,
  title={Natural language watermarking: Design, analysis, and a proof-of-concept implementation},
  author={Atallah, Mikhail J and Raskin, Victor and Crogan, Michael and Hempelmann, Christian and Kerschbaum, Florian and Mohamed, Dina and Naik, Sanket},
  booktitle={Information Hiding: 4th International Workshop, IH 2001 Pittsburgh, PA, USA, April 25--27, 2001 Proceedings 4},
  pages={185--200},
  year={2001},
  organization={Springer}
}

@inproceedings{topkara2006words,
  title={Words are not enough: sentence level natural language watermarking},
  author={Topkara, Mercan and Topkara, Umut and Atallah, Mikhail J},
  booktitle={Proceedings of the 4th ACM international workshop on Contents protection and security},
  pages={37--46},
  year={2006}
}

@article{meral2009natural,
  title={Natural language watermarking via morphosyntactic alterations},
  author={Meral, Hasan Mesut and Sankur, B{\"u}lent and {\"O}zsoy, A Sumru and G{\"u}ng{\"o}r, Tunga and Sevin{\c{c}}, Emre},
  journal={Computer Speech \& Language},
  volume={23},
  number={1},
  pages={107--125},
  year={2009},
  publisher={Elsevier}
}

% Neural Rewriting-Based Watermarking

@inproceedings{abdelnabi2021adversarial,
  title={Adversarial watermarking transformer: Towards tracing text provenance with data hiding},
  author={Abdelnabi, Sahar and Fritz, Mario},
  booktitle={2021 IEEE Symposium on Security and Privacy (SP)},
  pages={121--140},
  year={2021},
  organization={IEEE}
}

@article{zhang2023remark,
  title={REMARK-LLM: A Robust and Efficient Watermarking Framework for Generative Large Language Models},
  author={Zhang, Ruisi and Hussain, Shehzeen Samarah and Neekhara, Paarth and Koushanfar, Farinaz},
  journal={arXiv preprint arXiv:2310.12362},
  year={2023}
}

@article{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  journal={Advances in neural information processing systems},
  volume={27},
  year={2014}
}

@inproceedings{
    jang2016categorical,
    title={Categorical Reparameterization with Gumbel-Softmax},
    author={Eric Jang and Shixiang Gu and Ben Poole},
    booktitle={International Conference on Learning Representations},
    year={2017},
    url={https://openreview.net/forum?id=rkE3y85ee}
}

@article{xu2024robust,
  title={Robust Multi-bit Text Watermark with LLM-based Paraphrasers},
  author={Xu, Xiaojun and Jia, Jinghan and Yao, Yuanshun and Liu, Yang and Li, Hang},
  journal={arXiv preprint arXiv:2412.03123},
  year={2024}
}

@article{lau2024waterfall,
  title={Waterfall: Framework for Robust and Scalable Text Watermarking},
  author={Lau, Gregory Kang Ruey and Niu, Xinyuan and Dao, Hieu and Chen, Jiangwei and Foo, Chuan-Sheng and Low, Bryan Kian Hsiang},
  journal={arXiv preprint arXiv:2407.04411},
  year={2024}
}



% Distillation-Based Watermarking.
@inproceedings{
    gu2024on,
    title={On the Learnability of Watermarks for Language Models},
    author={Chenchen Gu and Xiang Lisa Li and Percy Liang and Tatsunori Hashimoto},
    booktitle={The Twelfth International Conference on Learning Representations},
    year={2024},
    url={https://openreview.net/forum?id=9k0krNzvlV}
}

@article{xu2024learning,
  title={Learning to watermark llm-generated text via reinforcement learning},
  author={Xu, Xiaojun and Yao, Yuanshun and Liu, Yang},
  journal={arXiv preprint arXiv:2403.10553},
  year={2024}
}

@article{gloaguen2025robust,
  title={Robust LLM Fingerprinting via Domain-Specific Watermarks},
  author={Gloaguen, Thibaud and Staab, Robin and Jovanovi{\'c}, Nikola and Vechev, Martin},
  journal={arXiv preprint arXiv:2505.16723},
  year={2025}
}



@inproceedings{kirchenbauer2023watermark,
  title={A watermark for large language models},
  author={Kirchenbauer, John and Geiping, Jonas and Wen, Yuxin and Katz, Jonathan and Miers, Ian and Goldstein, Tom},
  booktitle={International Conference on Machine Learning},
  pages={17061--17084},
  year={2023},
  organization={PMLR}
}
@article{takezawa2023necessary,
  title={Necessary and sufficient watermark for large language models},
  author={Takezawa, Yuki and Sato, Ryoma and Bao, Han and Niwa, Kenta and Yamada, Makoto},
  journal={arXiv preprint arXiv:2310.00833},
  year={2023}
}
@inproceedings{fu2024watermarking,
  title={Watermarking conditional text generation for ai detection: Unveiling challenges and a semantic-aware watermark remedy},
  author={Fu, Yu and Xiong, Deyi and Dong, Yue},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={16},
  pages={18003--18011},
  year={2024}
}
@article{ren2023robust,
  title={A robust semantics-based watermark for large language model against paraphrasing},
  author={Ren, Jie and Xu, Han and Liu, Yiding and Cui, Yingqian and Wang, Shuaiqiang and Yin, Dawei and Tang, Jiliang},
  journal={arXiv preprint arXiv:2311.08721},
  year={2023}
}
@article{wang2025morphmark,
  title={Morphmark: Flexible adaptive watermarking for large language models},
  author={Wang, Zongqi and Gu, Tianle and Wu, Baoyuan and Yang, Yujiu},
  journal={arXiv preprint arXiv:2505.11541},
  year={2025}
}
@article{nemecek2025feasibility,
  title={The Feasibility of Topic-Based Watermarking on Academic Peer Reviews},
  author={Nemecek, Alexander and Jiang, Yuzhou and Ayday, Erman},
  journal={arXiv preprint arXiv:2505.21636},
  year={2025}
}

@article{liu2024adaptive,
  title={Adaptive text watermark for large language models},
  author={Liu, Yepeng and Bu, Yuheng},
  journal={arXiv preprint arXiv:2401.13927},
  year={2024}
}



@inproceedings{christ2024undetectable,
  title={Undetectable watermarks for language models},
  author={Christ, Miranda and Gunn, Sam and Zamir, Or},
  booktitle={The Thirty Seventh Annual Conference on Learning Theory},
  pages={1125--1139},
  year={2024},
  organization={PMLR}
}
@article{kuditipudi2023robust,
  title={Robust distortion-free watermarks for language models},
  author={Kuditipudi, Rohith and Thickstun, John and Hashimoto, Tatsunori and Liang, Percy},
  journal={arXiv preprint arXiv:2307.15593},
  year={2023}
}
@misc{yang2025enhancingwatermarkingqualityllms,
      title={Enhancing Watermarking Quality for LLMs via Contextual Generation States Awareness}, 
      author={Peiru Yang and Xintian Li and Wanchun Ni and Jinhua Yin and Huili Wang and Guoshun Nan and Shangguang Wang and Yongfeng Huang and Tao Qi},
      year={2025},
      eprint={2506.07403},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2506.07403}, 
}
@article{xu2024signal,
  title={Signal Watermark on Large Language Models},
  author={Xu, Zhenyu and Sheng, Victor S},
  journal={arXiv preprint arXiv:2410.06545},
  year={2024}
}

@article{hou2023semstamp,
  title={Semstamp: A semantic watermark with paraphrastic robustness for text generation},
  author={Hou, Abe Bohan and Zhang, Jingyu and He, Tianxing and Wang, Yichen and Chuang, Yung-Sung and Wang, Hongwei and Shen, Lingfeng and Van Durme, Benjamin and Khashabi, Daniel and Tsvetkov, Yulia},
  journal={arXiv preprint arXiv:2310.03991},
  year={2023}
}

@article{hou2024k,
  title={k-SemStamp: A clustering-based semantic watermark for detection of machine-generated text},
  author={Hou, Abe Bohan and Zhang, Jingyu and Wang, Yichen and Khashabi, Daniel and He, Tianxing},
  journal={arXiv preprint arXiv:2402.11399},
  year={2024}
}
@article{zhang2025cohemark,
  title={Cohemark: A novel sentence-level watermark for enhanced text quality},
  author={Zhang, Junyan and Liu, Shuliang and Liu, Aiwei and Gao, Yubo and Li, Jungang and Gu, Xiaojie and Hu, Xuming},
  journal={arXiv preprint arXiv:2504.17309},
  year={2025}
}

@article{yujian2007normalized,
  title={A normalized Levenshtein distance metric},
  author={Yujian, Li and Bo, Liu},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={29},
  number={6},
  pages={1091--1095},
  year={2007},
  publisher={IEEE}
}

% New papers added
@article{chen2025clmtracing,
  title={CLMTracing: Black-box User-level Watermarking for Code Language Model Tracing},
  author={Chen, Zhen and Zhang, Yuxuan and Li, Mingjie and Fang, Xiaojun and Lin, Jia and Zhang, Zhenyu and Zhang, Tao and Sun, Lichao and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2509.13982},
  year={2025}
}

@article{zhang2024explanation,
  title={Explanation as a Watermark: Towards Harmless and Multi-bit Model Ownership Verification via Watermarking Feature Attribution},
  author={Zhang, Yifeng and Sun, Yifan and Lu, Yunchao and Zhang, Xiangyu and Wu, Baoyuan},
  journal={arXiv preprint arXiv:2405.04825},
  year={2024}
}

@article{liu2025robust,
  title={Robust Data Watermarking in Language Models by Injecting Fictitious Knowledge},
  author={Liu, Runlin and Zhao, Yuxin and Chen, Jialuo and Ma, Xingjun},
  journal={arXiv preprint arXiv:2503.04036},
  year={2025}
}

@article{zhang2025fitprint,
  title={FIT-Print: Towards False-claim-resistant Model Ownership Verification via Targeted Fingerprint},
  author={Zhang, Yichi and Xu, Xuefei Ning and Liang, Han and Zha, Sheng and Wang, Zhangyang},
  journal={arXiv preprint arXiv:2501.15509},
  year={2025}
}

@article{liu2024sos,
  title={SOS! Soft Prompt Attack Against Open-Source Large Language Models},
  author={Liu, Zhihao and Zhang, Zheng and Li, Xiangyu and Ren, Kui and Zhang, Xiangyu},
  journal={arXiv preprint arXiv:2407.03160},
  year={2024}
}

@inproceedings{xu2025esf,
  title={ESF: Efficient Sensitive Fingerprinting for Black-Box Tamper Detection of Large Language Models},
  author={Xu, Peng and Zhao, Mengfei and Wang, Han and Wang, Ruoyu and Yang, Yiming and Li, Qian and Song, Jia},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2025},
  pages={9876--9892},
  year={2025}
}

@article{li2024your,
  title={Your Large Language Models Are Leaving Fingerprints},
  author={Li, Jiahao and Zhang, Kaiwen and Wu, Jiayu and Chen, Yufei and Zhang, Tong and Li, Bo and Zhang, Tianwei},
  journal={arXiv preprint arXiv:2405.14057},
  year={2024}
}

@article{wei2025behavioral,
  title={Behavioral Fingerprinting of Large Language Models},
  author={Wei, Zeyu and Zhang, Hao and Wang, Junjie and Zhang, Yue and Liu, Zhiyuan and Lin, Yankai},
  journal={arXiv preprint arXiv:2509.04504},
  year={2025}
}

@article{liu2025fdllm,
  title={FDLLM: A Dedicated Detector for Black-Box LLMs Fingerprinting},
  author={Liu, Han and Wang, Jinpeng and Zhou, Xin and Zhang, Hui and Xu, Shujian and Yang, Zhendong},
  journal={arXiv preprint arXiv:2501.16029},
  year={2025}
}